{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import local modules\n",
    "from data_loading import get_or_create_database,load_dataset,load_paper_data,get_paper,get_papers\n",
    "from preprocessor import ArXivPreprocessor\n",
    "from vectorizer import Vectorizer,process_existing_papers,process_papers_batch\n",
    "from rag import RAGAgent\n",
    "\n",
    "\n",
    "#import third part modules\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import tool\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "#import standard modules\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/bababako/side_projects/llm_projects/venv_rag/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "# initialize connection to pinecone\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Pinecone API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
    "\n",
    "#initialize encoder, pc, and spec. get dimension of the encoder as well\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"  # us-east-1\n",
    ")\n",
    "\n",
    "dims = len(encoder([\"test\"])[0])\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 41}},\n",
       " 'total_vector_count': 41}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#initialize the index. \n",
    "index_name = \"dsny-rag-quick\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # dimensionality of embed 3\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a dataset (e.g., JSON, CSV, or unstructured text files) that includes a mix of structured and unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\").shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2005.05257',\n",
       " 'title': 'A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering',\n",
       " 'summary': 'Legislation can be viewed as a body of prescriptive rules expressed in\\nnatural language. The application of legislation to facts of a case we refer to\\nas statutory reasoning, where those facts are also expressed in natural\\nlanguage. Computational statutory reasoning is distinct from most existing work\\nin machine reading, in that much of the information needed for deciding a case\\nis declared exactly once (a law), while the information needed in much of\\nmachine reading tends to be learned through distributional language statistics.\\nTo investigate the performance of natural language understanding approaches on\\nstatutory reasoning, we introduce a dataset, together with a legal-domain text\\ncorpus. Straightforward application of machine reading models exhibits low\\nout-of-the-box performance on our questions, whether or not they have been\\nfine-tuned to the legal domain. We contrast this with a hand-constructed\\nProlog-based system, designed to fully solve the task. These experiments\\nsupport a discussion of the challenges facing statutory reasoning moving\\nforward, which we argue is an interesting real-world task that can motivate the\\ndevelopment of models able to utilize prescriptive rules specified in natural\\nlanguage.',\n",
       " 'source': 'http://arxiv.org/pdf/2005.05257',\n",
       " 'authors': 'Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme',\n",
       " 'categories': 'cs.CL',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20200511',\n",
       " 'updated': '20200812',\n",
       " 'content': '0 2 0 2\\ng u A 2 1 ] L C . s c [\\n3 v 7 5 2 5 0 . 5 0 0 2 : v i X r a\\n# A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering\\nNils Holzenberger Johns Hopkins University Baltimore, Maryland, USA nilsh@jhu.edu\\nAndrew Blair-Stanek U. of Maryland Carey School of Law Baltimore, Maryland, USA Johns Hopkins University Baltimore, Maryland, USA ablair-stanek@law.umaryland.edu\\nBenjamin Van Durme Johns Hopkins University Baltimore, Maryland, USA vandurme@cs.jhu.edu\\nABSTRACT Legislation can be viewed as a body of prescriptive rules expressed in natural language. The application of legislation to facts of a case we refer to as statutory reasoning, where those facts are also ex- pressed in natural language. Computational statutory reasoning is distinct from most existing work in machine reading, in that much of the information needed for deciding a case is declared exactly once (a law), while the information needed in much of machine read- ing tends to be learned through distributional language statistics. To investigate the performance of natural language understanding approaches on statutory reasoning, we introduce a dataset, together with a legal-domain text corpus. Straightforward application of ma- chine reading models exhibits low out-of-the-box performance on our questions, whether or not they have been fine-tuned to the legal domain. We contrast this with a hand-constructed Prolog-based system, designed to fully solve the task. These experiments support a discussion of the challenges facing statutory reasoning moving forward, which we argue is an interesting real-world task that can motivate the development of models able to utilize prescriptive rules specified in natural language.\\nCCS CONCEPTS â\\x80¢ Applied computing â\\x86\\x92 Law; â\\x80¢ Computing methodologies â\\x86\\x92 Natural language processing; Knowledge representation and reason- ing.\\n# KEYWORDS Law, NLP, Reasoning, Prolog\\nsources, e.g. textbooks, and connecting high-performance symbolic solvers with large-scale language models.\\nIn parallel, models have begun to consider task definitions like Machine Reading (MR) [46] and Recognizing Textual Entailment (RTE) [15, 16] as not requiring the use of explicit structure. Instead, the problem is cast as one of mapping inputs to high-dimensional, dense representations that implicitly encode meaning [18, 45], and are employed in building classifiers or text decoders, bypassing classic approaches to symbolic inference.\\nThis work is concerned with the problem of statutory reasoning [62, 66]: how to reason about an example situation, a case, based on complex rules provided in natural language. In addition to the reasoning aspect, we are motivated by the lack of contemporary systems to suggest legal opinions: while there exist tools to aid lawyers in retrieving relevant documents for a given case, we are unaware of any strong capabilities in automatic statutory reasoning. Our contributions, summarized in Figure 2, include a novel dataset based on US tax law, together with test cases (Section 2). Decades-old work in expert systems could solve problems of the sort we construct here, based on manually derived rules: we repli- cate that approach in a Prolog-based system that achieves 100% accuracy on our examples (Section 3). Our results demonstrate that straightforward application of contemporary Machine Read- ing models is not sufficient for our challenge examples (Section 5), whether or not they were adapted to the legal domain (Section 4). This is meant to provoke the question of whether we should be concerned with: (a) improving methods in semantic parsing in order to replace manual transduction into symbolic form; or (b) improving machine reading methods in order to avoid explicit symbolic solvers. We view this work as part of the conversation including recent work in multi-hop inference [61], where our task is more domain-specific but potentially more challenging.\\n1 INTRODUCTION Early artificial intelligence research focused on highly-performant, narrow-domain reasoning models, for instance in health [37, 40, 54] and law [30, 38]. Such expert systems relied on hand-crafted inference rules and domain knowledge, expressed and stored with the formalisms provided by databases [21]. The main bottleneck of this approach is that experts are slow in building such knowledge bases and exhibit imperfect recall, which motivated research into models for automatic information extraction (e.g. Lafferty et al. [36]). Systems for large-scale automatic knowledge base construction have improved (e.g. Etzioni et al. [20], Mitchell et al. [41]), as well as systems for sentence level semantic parsing [64]. Among others, this effort has led to question-answering systems for games [22] and, more recently, for science exams [14, 23, 27]. The challenges include extracting ungrounded knowledge from semi-structured\\n2 DATASET Here, we describe our main contribution, the StAtutory Reason- ing Assessment dataset (SARA): a set of rules extracted from the statutes of the US Internal Revenue Code (IRC), together with a set of natural language questions which may only be answered correctly by referring to the rules1.\\nThe IRC2 contains rules and definitions for the imposition and calculation of taxes. It is subdvided into sections, which in general,\\n1The dataset can be found under https://nlp.jhu.edu/law/ 2https://uscode.house.gov/browse/prelim@title26&edition=prelim\\nÂ§63. Taxable income Binary defined Â§2. Definitions and [special rules Â§1. Tax imposed Numerical (a) Married individuals filing joint returns land surviving spouses ns There is hereby imposed fion the jon the taxable income [ng of (1) every married individual...\\nFigure 1: Sample cases from our dataset. The questions can be answered by applying the rules contained in the statutes to the context. \\nus 26 case. law Prolog rules Tax Corpus Tax Vectors Legal BERT ae Â§ dodnaindn aa dodn\\nFigure 2: Resources. Corpora on the left hand side were used to build the datasets and models on the right hand side.\\ndefine one or more terms: section 3306 defines the terms employ- ment, employer and wages, for purposes of the federal unemploy- ment tax. Sections are typically structured around a general rule, followed by a number of exceptions. Each section and its subsec- tions may be cast as a predicate whose truth value can be checked against a state of the world. For instance, subsection 7703(a)(2):\\nan individual legally separated from his spouse under a decree of divorce or of separate maintenance shall not be considered as married\\ncan be checked given an individual.\\nSlots are another major feature of the law. Each subsection refers to a certain number of slots, which may be filled by existing entities (in the above, individual, spouse, and decree of divorce or of separate maintenance). Certain slots are implicitly filled: Â§7703(a)(1) and (b)(3) mention a â\\x80\\x9cspouse\", which must exist since the â\\x80\\x9cindividual\" is married. Similarly, slots which have been filled earlier in the section may be referred to later on. For instance, â\\x80\\x9chousehold\" is mentioned for the first time in Â§7703(b)(1), then again in Â§7703(b)(2) and in Â§7703(b)(3). Correctly resolving slots is a key point in successfully applying the law.\\nOverall, the IRC can be framed as a set of predicates formulated in human language. The language used to express the law has an open texture [29], which makes it particularly challenging for a computer- based system to determine whether a subsection applies, and to identify and fill the slots mentioned. This makes the IRC an excellent corpus to build systems that reason with rules specified in natural language, and have good language understanding capabilities.\\n2.1 Statutes and test cases As the basis of our set of rules, we selected sections of the IRC well-supported by Treasury Regulations, covering tax on individu- als (Â§1), marriage and other legal statuses (Â§2, 7703), dependents (Â§152), tax exemptions and deductions (Â§63, 68, 151) and employ- ment (Â§3301, 3306). We simplified the sections to (1) remove highly specific sections (e.g. those concerning the employment of sailors) in order to keep the statutes to a manageable size, and (2) ensure that the sections only refer to sections from the selected subset. For ease of comparison with the original statutes, we kept the original numbering and lettering, with no adjustment for removed sections. For example, there is a section 63(d) and a section 63(f), but no section 63(e). We assumed that any taxable year starts and ends at the same time as the corresponding calendar year.\\nFor each subsection extracted from the statutes, we manually created two paragraphs in natural language describing a case, one where the statute applies, and one where it does not. These snippets, formulated as a logical entailment task, are meant to test a systemâ\\x80\\x99s understanding of the statutes, as illustrated in Figure 1. The cases were vetted by a law professor for coherence and plausibility. For the purposes of machine learning, the cases were split into 176 train and 100 test samples, such that (1) each pair of positive and negative cases belongs to the same split, and (2) each section is split between train and test in the same proportions as the overall split. Since tax legislation makes it possible to predict how much tax a person owes, we created an additional set of 100 cases where the task is to predict how much tax someone owes. Those cases were created by randomly mixing and matching pairs of cases from the first set of cases, and resolving inconsistencies manually. Those cases are no longer a binary prediction task, but a task of predicting an integer. The prediction results from taking into account the entirety of the statutes, and involves basic arithmetic. The 100 cases were randomly split into 80 training and 20 test samples.\\nBecause the statutes were simplified, the answers to the cases are not those that would be obtained with the current version of the IRC. Some of the IRC counterparts of the statutes in our dataset have been repealed, amended, or adjusted to reflect inflation.\\n2.2 Key features of the corpus While the corpus is based on a simplification of the Internal Rev- enue Code, care was taken to retain prominent features of US law. We note that the present task is only one aspect of legal reason- ing, which in general involves many more modes of reasoning, in particular interpreting regulations and prior judicial decisions. The following features are quantified in Tables 1 to 4.\\nReasoning with time. The timing of events (marriage, retirement, income...) is highly relevant to determining whether certain sections apply, as tax is paid yearly. In total, 62 sections refer to time. Some sections require counting days, as in Â§7703(b)(1):\\nCross-references Within the section To another section Explicit 30 34 Implicit 25 44\\n# Table 1: Number of subsections containing cross-references\\nmin max 6 6 Table 2: Statistics about the tree structure of the statutes\\na household which constitutes for more than one-half of the taxable year the principal place of abode of a child or taking into account the absolute point in time as in Â§63(c)(7): In the case of a taxable year beginning after December 31, 2017, and before January 1, 2026-\\nExceptions and substitutions. Typically, each section of the IRC starts by defining a general case and then enumerates a number of exceptions to the rule. Additionally, some rules involve applying a rule after substituting terms. A total of 50 sections formulate an exception or a substitution. As an example, Â§63(f)(3):\\nIn the case of an individual who is not married and is not a surviving spouse, paragraphs (1) and (2) shall be applied by substituting â\\x80\\x9c$750\" for â\\x80\\x9c$600\".\\nNumerical reasoning. Computing tax owed requires knowledge of the basic arithmetic operations of adding, subtracting, multiplying, dividing, rounding and comparing numbers. 55 sections involve numerical reasoning. The operation to be used needs to be parsed out of natural text, as in Â§1(c)(2):\\n$3,315, plus 28% of the excess over $22,100 if the taxable income is over $22,100 but not over $53,500\\nCross-references. Each section of the IRC will typically reference other sections. Table 1 shows how this feature was preserved in our dataset. There are explicit references within the same section, as in Â§7703(b)(1):\\nan individual who is married (within the meaning of subsection (a)) and who files a separate return\\nexplicit references to another section, as in Â§3301:\\nThere is hereby imposed on every employer (as defined in section 3306(a)) for each calendar year an excise tax and implicit references, as in Â§151(a), where â\\x80\\x9ctaxable income\" is defined in Â§63:\\nthe exemptions provided by this section shall be allowed as deductions in computing taxable income.\\nCommon sense knowledge. Four concepts, other than time, are left undefined in our statutes: (1) kinship, (2) the fact that a marriage ends if either spouse dies, (3) if an event has not ended, then it is ongoing; if an event has no start, it has been true at any time before it ends; and some events are instantaneous (e.g. payments ), (4) a personâ\\x80\\x99s gross income is the sum of all income and payments received by that person.\\nHierarchical structure. Law statutes are divided into sections, themselves divided into subsections, with highly variable depth and structure. This can be represented by a tree, with a special ROOT node of depth 0 connecting all the sections. This tree contains 132 leaves and 193 nodes (node includes leaves). Statistics about depth are in Table 2.\\nVocabulary size Sentence length (in words) Case length (in sentences) Case length (in words) Section length train test statutes combined train test combined train test combined sentences words train test min max 138 34 88 138 9 7 9 179 81 179 16 1151 4 4 1 1 1 2 1 17 17 17 2 62 867 535 avg 12.3 11.6 16.5 12.7 4.2 3.8 4.1 48.5 41.6 46.3 8.3 488.9 statutes combined 768 1596 stddev median 11 10 12.5 11 4 4 4 43 38 41 9 549 9.1 4.5 14.9 9.5 1.7 1.3 1.6 22.2 14.7 20.3 4.7 310.4\\nTable 3: Language statistics. The word â\\x80\\x9ccombinedâ\\x80\\x9d means merging the corpora mentioned above it.\\ntrain test combined min 0 0 0 max 2,242,833 243,097 2,242,833 average 85,804.86 65,246.50 81,693.19 stddev 258,179.30 78,123.13 233,695.33 median 15,506.50 26,874.00 17,400.50 Table 4: Answers to numerical questions (in $).\\n3 PROLOG SOLVER It has been shown that subsets of statutes can be expressed in first- order logic, as described in Section 6. As a reaffirmation of this, and as a topline for our task, we have manually translated the statutes into Prolog rules and the cases into Prolog facts, such that each case can be answered correctly by a single Prolog query3. The Prolog rules were developed based on the statutes, meaning that the Prolog code clearly reflects the semantics of the textual form, as in Gunning et al. [27]. This is primarily meant as a proof that a carefully crafted reasoning engine, with perfect natural language understanding, can solve this dataset. There certainly are other ways of representing this given set of statutes and cases. The point of this dataset is not to design a better Prolog system, but to help the development of language understanding models capable of reasoning.\\n3.1 Statutes Each subsection of the statutes was translated with a single rule, true if the section applies, false otherwise. In addition, subsections define slots that may be filled and reused in other subsections, as described in Section 2. To solve this coreference problem, any term appearing in a subsection and relevant across subsections is turned into an argument of the Prolog rule. The corresponding variable may then be bound during the execution of a rule, and reused in a rule executed later. Unfilled slots correspond to unbound variables. To check whether a given subsection applies, the Prolog sys- tem needs to rely on certain predicates, which directly reflect the facts contained in the natural language descriptions of the cases. For instance, how do we translate Alice and Bob got married on January 24th, 1993 into code usable by Prolog? We rely on a set of 61 predicates, following neo-davidsonian semantics [9, 17, 42]. The level of detail of these predicates is based on the granularity of the statutes themselves. Anything the statutes do not define, and which is typically expressed with a single word, is potentially\\n3The Prolog program can be found under https://nlp.jhu.edu/law/\\nsuch a predicate: marriage, residing somewhere, someone paying someone else, etc. The example above is translated in Figure 3.\\n3.2 Cases The natural lan- guage description of each case was manually translated into the facts men- tioned above. The question or log- ical entailment prompt was translated into a Prolog query. For instance, Section 7703(b)(3) applies to Alice maintaining her home for the year 2018. translates to s7703_b_3(alice,home,2018). and How much tax does Alice have to pay in 2017? translates to tax(alice,2017,Amount).\\nIn the broader context of computational statutory reasoning, the Prolog solver has three limitations. First, producing it requires domain experts, while automatic generation is an open question. Second, translating natural language into facts requires semantic parsing capabilities. Third, small mistakes can lead to catastrophic failure. An orthogonal approach is to replace logical operators and explicit structure with high-dimensional, dense representations and real-valued functions, both learned using distributional statistics. Such a machine learning-based approach can be adapted to new legislation and new domains automatically.\\n4 LEGAL NLP As is commonly done in MR, we pretrained our models using two unsupervised learning paradigms on a large corpus of legal text.\\n4.1 Text corpus We curated a corpus consisting solely of freely-available tax law documents with 147M tokens. The first half is drawn from cas [1], a project of Harvardâ\\x80\\x99s Law Library that scanned and OCRâ\\x80\\x99ed many of the libraryâ\\x80\\x99s case-law reporters, making the text available upon request to researchers. The main challenge in using this resource is that it contains 1.7M U.S. federal cases, only a small percentage of which are on tax law (as opposed to criminal law, breach of contract, bankruptcy, etc.). Classifying cases by area is a non-trivial problem [55], and tax-law cases are litigated in many different courts. We used the heuristic of classifying a case as being tax-law if it met one of the following criteria: the Commissioner of Internal Revenue was a party; the case was decided by the U.S. Tax Court; or, the case was decided by any other federal court, other than a trade tribunal, with the United States as a party, and with the word tax appearing in the first 400 words of the caseâ\\x80\\x99s written opinion.\\nThe second half of this corpus consists of IRS private letter rul- ings and unpublished U.S. Tax Court cases. IRS private letter rulings are similar to cases, in that they apply tax law to one taxpayerâ\\x80\\x99s facts; they differ from cases in that they are written by IRS attorneys (not judges), have less precedential authority than cases, and redact names to protect taxpayer privacy. Unpublished U.S. Tax Court cases are viewed by the judges writing them as less important than those worthy of publication. These were downloaded as PDFs from the IRS and Tax Court websites, OCRâ\\x80\\x99ed with tesseract if needed, and otherwise cleaned.\\n4.2 Tax vectors Before training a word2vec model [39] on this corpus, we did two tax-specific preprocessing steps to ensure that semantic units re- mained together. First, we put underscores between multi-token collocations that are tax terms of art, defined in either the tax code, Treasury regulations, or a leading tax-law dictionary. Thus, â\\x80\\x9csurviv- ing spouse\" became the single token â\\x80\\x9csurviving_spouse\". Second, we turned all tax code sections and Treasury regulations into a single token, stripped of references to subsections, subparagraphs, and subclauses. Thus, â\\x80\\x9cTreas. Reg. Â§1.162-21(b)(1)(iv)\" became the single token â\\x80\\x9csec_1_162_21\". The vectors were trained at 500 dimen- sions using skip-gram with negative sampling. A window size of 15 was found to maximize performance on twelve human-constructed analogy tasks.\\n4.3 Legal BERT We performed further training of BERT [18], on a portion of the full case.law corpus, including both state and federal cases. We did not limit the training to tax cases. Rather, the only cases excluded were those under 400 characters (which tend to be summary orders with little semantic content) and those before 1970 (when judicial writing styles had become recognizably modern). We randomly selected a subset of the remaining cases, and broke all selected cases into chunks of exactly 510 tokens, which is the most BERTâ\\x80\\x99s architecture can handle. Any remaining tokens in a selected case were discarded. Using solely the masked language model task (i.e. not next sentence prediction), starting from Bert-Base-Cased, we trained on 900M tokens.\\nThe resulting Legal BERT has the exact same architecture as Bert-Base-Cased but parameters better attuned to legal tasks. We applied both models to the natural language questions and answers in the corpus we introduce in this paper. While Bert-Base-Cased had a perplexity of 14.4, Legal BERT had a perplexity of just 2.7, suggesting that the further training on 900M tokens made the model much better adapted to legal queries.\\nWe also probed how this further training impacted ability to handle fine-tuning on downstream tasks. The downstream task we chose was identifying legal terms in case texts. For this task, we defined legal terms as any tokens or multi-token collocations that are defined in Blackâ\\x80\\x99s Law Dictionary [25], the premier legal dictio- nary. We split the legal terms into training/dev/test splits. We put a 4-layer fully-connected MLP on top of both Bert-Base-Cased and Legal BERT, where the training objective was B-I-O tagging of tokens in 510-token sequences. We trained both on a set of 200M tokens randomly selected from case.law cases not previ- ously seen by the model and not containing any of the legal terms in dev or test, with the training legal terms tagged using string comparisons. We then tested both fine-tuned modelsâ\\x80\\x99 ability to identify legal terms from the test split in case law. The model based on Bert-Base-Cased achieved F1 = 0.35, whereas Legal BERT achieved F1 = 0.44. As a baseline, two trained lawyers given the same task on three 510-token sequences each achieved F1 = 0.26. These results indicate that Legal BERT is much better adapted to the legal domain than Bert-Base-Cased. Blackâ\\x80\\x99s Law Dictionary has well- developed standards for what terms are or are not included. BERT models learn those standards via the train set, whereas lawyers are not necessarily familiar with them. In addition, pre-processing\\ndropped some legal terms that were subsets of too many others, which the lawyers tended to identify. This explains how BERT- based models could outperform trained humans.\\n5 EXPERIMENTS 5.1 BERT-based models In the following, we frame our task as textual entailment and numer- ical regression. A given entailment prompt q mentions the relevant subsection (as in Figure 1)4. We extract s, the text of the relevant subsection, from the statutes. In q, we replace Section XYZ applies with This applies. We feed the string â\\x80\\x9c[CLS] + s + [SEP] + q + c + [SEP]\", where â\\x80\\x9c+\" is string concatenation, to BERT [18]. Let r be the vector representation of the token [CLS] in the final layer. The answer (entailment or contradiction) is predicted as Ð´(Î¸1 Â· r ) where Î¸1 is a learnable parameter and Ð´ is the sigmoid function. For numerical questions, all statutes have to be taken into account, which would exceed BERTâ\\x80\\x99s length limit. We encode â\\x80\\x9c[CLS] all [SEP] + q + c + [SEP]\" into r and predict the answer as Âµ + Ï\\x83Î¸2 Â· r where Î¸2 is a learned parameter, and Âµ and Ï\\x83 are the mean and standard deviation of the numerical answers on the training set.\\nFor entailment, we use a cross-entropy loss, and evaluate the models using accuracy. We frame the numerical questions as a taxpayer having to compute tax owed. By analogy with the concept of â\\x80\\x9csubstantial understatement of income taxâ\\x80\\x9d from Â§6662(d), we define â\\x88\\x86(y, Ë\\x86y) = max(0.1y,5000) where y is the true amount of tax owed, and Ë\\x86y is the taxpayerâ\\x80\\x99s prediction. The case â\\x88\\x86(y, Ë\\x86y) â\\x89¥ 1 corresponds to a substantial over- or understatement of tax. We compute the fraction of predictions Ë\\x86y such that â\\x88\\x86(y, Ë\\x86y) < 1 and report that as numerical accuracy.5 The loss function used is:\\nL= Y yilog gi + (1 - yi) log(1 - gi) + Y* max(A(yi, Gi) - 1,0) ieh iely\\nwhere I1 (resp. I2) is the set of entailment (resp. numerical) ques- tions, yi is the ground truth output, and Ë\\x86yi is the modelâ\\x80\\x99s output. We use Adam [34] with a linear warmup schedule for the learning rate. We freeze BERTâ\\x80\\x99s parameters, and experiment with unfreezing BERTâ\\x80\\x99s top layer. We select the final model based on early stopping with a random 10% of the training examples reserved as a dev set. The best performing model for entailment and for numerical questions are selected separately, during a hyperparameter search around the recommended setting (batch size=32, learning rate=1e- 5). To check for bias in our dataset, we drop either the statute, or the context and the statute, in which case we predict the answer from BERTâ\\x80\\x99s representation for â\\x80\\x9c[CLS] + c + [SEP] + q + [SEP]\" or â\\x80\\x9c[CLS] + q + [SEP]\", whichever is relevant.\\n5.2 Feedforward models We follow Arora et al. [2] to embed strings into vectors, with smoothing parameter equal to 10â\\x88\\x923. We use either tax vectors de- scribed in Section 4 or word2vec vectors [39]. We estimate unigram counts from the corpus used to build the tax vectors, or the train- ing set, whichever is relevant. For a given context c and question\\n4The code for these experiments can be found under https://github.com/SgfdDttt/sara 5For a company, a goal would be to have 100% accuracy (resulting in no tax penalties) while paying the lowest amount of taxes possible (giving them something of an interest- free loan, even if the IRS eventually collects the understated tax).\\nInputs - question context statutes statutes question context statutes statutes question context statutes question context statutes question context statutes question context statutes Table 5: Test set scores. We report the 90% confidence inter- val. All confidence intervals for entailment round to 8.3%.\\nor prompt q, we retrieve relevant subsection s as above. Using Arora et al. [2], s is mapped to vector vs , and (c, q) to vc+q . Let r = [vs , vq+c , |vs â\\x88\\x92 vc+q |, vs â\\x8a\\x99 vc+q ] where [a, b] is the concate- nation of a and b, |.| is the element-wise absolute value, and â\\x8a\\x99 is the element-wise product. The answer is predicted as Ð´(Î¸1 Â· f (r )) or Âµ + Ï\\x83Î¸2 Â· f (r ), as above, where f is a feed-forward neural net- work. We use batch normalization between each layer of the neural network [31]. As above, we perform ablation experiments, where we drop the statute, or the context and the statute, in which case r is replaced by vc+q or vq . We also experiment with f being the identity function (no neural network). Training is otherwise done as above, but without the warmup schedule.\\n5.3 Results We report the accuracy on the test set (in %) in Table 5. In our ab- lation experiments, â\\x80\\x9cquestion\" models have access to the question only, â\\x80\\x9ccontext\" to the context and question, and â\\x80\\x9cstatute\" to the statutes, context and question. For entailment, we use a majority baseline. For the numerical questions, we find the constant that minimizes the hinge loss on the training set up to 2 digits: $11,023. As a check, we swapped in the concatenation of the RTE datasets of Bentivogli et al. [5], Dagan et al. [16], Giampiccolo et al. [26], Haim et al. [28], and achieved 73.6% accuracy on the dev set with BERT, close to numbers reported in Wang et al. [59]. BERT was trained on Wikipedia, which contains snippets of law text: see article United States Code and links therefrom, especially Internal Revenue Code. Overall, models perform comparably to the baseline, independent of the underlying method. Performance remains mostly unchanged when dropping the statutes or statutes and context, meaning that models are not utilizing the statutes. Adapting BERT or word vec- tors to the legal domain has no noticeable effect. Our results suggest that performance will not be improved through straightforward application of a large-scale language model, unlike it is on other datasets: Raffel et al. [45] achieved 94.8% accuracy on COPA [49]\\nusing a large-scale multitask Transformer model, and BERT pro- vided a huge jump in performance on both SQuAD 2.0 [46] (+8.2 F1) and SWAG [63] (+27.1 percentage points accuracy) datasets as compared to predecessor models, pre-trained on smaller datasets. Here, we focus on the creation of resources adapted to the legal domain, and on testing off-the-shelf and historical solutions. Future work will consider specialized reasoning models.\\n6 RELATED WORK There have been several efforts to translate law statutes into expert systems. Oracle Policy Automation has been used to formalize rules in a variety of contexts. TAXMAN [38] focuses on corporate reorga- nization law, and is able to classify a case into three different legal types of reorganization, following a theorem-proving approach. Sergot et al. [52] translate the major part of the British National- ity Act 1981 into around 150 rules in micro-Prolog, proving the suitability of Prolog logic to express and apply legislation. Bench- Capon et al. [4] further discuss knowledge representation issues. Closest to our work is Sherman [53], who manually translated part of Canadaâ\\x80\\x99s Income Tax Act into a Prolog program. To our knowl- edge, the projects cited did not include a dataset or task that the programs were applied to. Other works have similarly described the formalization of law statutes into rule-based systems [24, 30, 32, 51]. Yoshioka et al. [62] introduce a dataset of Japanese statute law and its English translation, together with questions collected from the Japanese bar exam. To tackle these two tasks, Kim et al. [33] investigate heuristic-based and machine learning-based methods. A similar dataset based on the Chinese bar exam was released by Zhong et al. [66]. Many papers explore case-based reasoning for law, with expert systems [43, 56], human annotations [8] or automatic annotations [3] as well as transformer-based methods [44]. Some datasets are concerned with very specific tasks, as in tagging in contracts [10], classifying clauses [11], and classification of documents [12] or single paragraphs [6]. Ravichander et al. [47] have released a dataset of questions about privacy policies, elicited from turkers and answered by legal experts. Saeidi et al. [50] frame the task of statutory reasoning as a dialog between a user and a dialog agent. A single rule, with or without context, and a series of followup questions are needed to answer the original question. Contrary to our dataset, rules are isolated from the rest of the body of rules, and followup questions are part of the task.\\nClark et al. [14] describe a decades-long effort to answer science exam questions stated in natural language, based on descriptive knowledge stated in natural language. Their system relies on a variety of NLP and specialized reasoning techniques, with their most significant gains recently achieved via contextual language modeling. This line of work is the most related in spirit to where we believe research in statutory reasoning should focus. An interesting contrast is that while scientific reasoning is based on understanding the physical world, which in theory can be informed by all manner of evidence beyond texts, legal reasoning is governed by human- made rules. The latter are true by virtue of being written down and agreed to, and are not discovered through evidence and a scientific process. Thus, statutory reasoning is an exceptionally pure instance of a reasoner needing to understand prescriptive language.\\nWeston et al. [60] introduced a set of prerequisite toy tasks for AI systems, which require some amount of reasoning and common\\nsense knowledge. Contrary to the present work, the types of ques- tion in the train and test sets are highly related, and the vocabulary overlap is quite high. Numeric reasoning appears in a variety of MR challenges, such as in DROP [19].\\nUnderstanding procedural language â\\x80\\x93 knowledge needed to per- form a task â\\x80\\x93 is related to the problem of understanding statutes, and so we provide a brief description of some example investiga- tions in that area. Zhang et al. [65] published a dataset of how-to instructions, with human annotations defining key attributes (actee, purpose...) and models to automatically extract the attributes. Simi- larly, Chowdhury et al. [13] describe a dataset of human-elicited procedural knowledge, and WambsganÃ\\x9f and Fromm [58] automati- cally detect repair instructions from posts on an automotive forum. Branavan et al. [7] employed text from an instruction manual to improve the performance of a game-playing agent.\\n7 CONCLUSION We introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task. Our hand- built solver contrasts with our baselines based on current NLP approaches, even when we adapt them to the legal domain.\\nThe intersection between NLP and the legal domain is a growing area of research [3, 11, 33, 35, 48], but with few large-scale system- atic resources. Thus, in addition to the exciting challenge posed by statutory reasoning, we also intend this paper to be a contribution to legal-domain natural language processing.\\nGiven the poor out-of-the box performance of otherwise very powerful models, this dataset, which is quite small compared to typ- ical MR resources, raises the question of what the most promising direction of research would be. An important feature of statutory reasoning is the relative difficulty and expense in generating care- fully constructed training data: legal texts are written for and by lawyers, who are cost-prohibitive to employ in bulk. This is un- like most instances of MR where everyday texts can be annotated through crowdsourcing services. There are at least three strategies open to the community: automatic extraction of knowledge graphs from text with the same accuracy as we did for our Prolog solver [57]; improvements in MR to be significantly more data efficient in training; or new mechanisms for the efficient creation of training data based on pre-existing legal cases.\\nGoing forward, we hope our resource provides both (1) a bench- mark for a challenging aspect of natural legal language processing as well as for machine reasoning, and (2) legal-domain NLP models useful for the research community.\\nREFERENCES [1] 2019. Caselaw Access Project. http://case.law [2] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat\\nbaseline for sentence embeddings. (2016).\\n[3] Kevin D Ashley and Stefanie BrÃ¼ninghaus. 2009. Automatically classifying case texts and predicting outcomes. Artificial Intelligence and Law 17, 2 (2009), 125â\\x80\\x93165.\\n[4] Trevor JM Bench-Capon, Gwen O Robinson, Tom W Routen, and Marek J Sergot. 1987. Logic programming for large scale applications in law: A formalisation of supplementary benefit legislation. In Proceedings of the 1st international conference on Artificial intelligence and law. 190â\\x80\\x93198.\\n[5] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The Fifth PASCAL Recognizing Textual Entailment Challenge.. In TAC.\\n[6] Carlo Biagioli, Enrico Francesconi, Andrea Passerini, Simonetta Montemagni, and Claudia Soria. 2005. Automatic semantics extraction in law documents. In Proceedings of the 10th international conference on Artificial intelligence and law. 133â\\x80\\x93140.\\n[7] SRK Branavan, David Silver, and Regina Barzilay. 2012. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence Research 43 (2012), 661â\\x80\\x93704.\\n[8] Stefanie Bruninghaus and Kevin D Ashley. 2003. Predicting outcomes of case based legal arguments. In Proceedings of the 9th international conference on Artifi- cial intelligence and law. 233â\\x80\\x93242.\\n[9] Hector Neri CastaÃ±eda. 1967. Comment on D. Davidsonâ\\x80\\x99s â\\x80\\x9cThe logical forms of action sentencesâ\\x80\\x9d. The Logic of Decision and Action (1967).\\n[10] Ilias Chalkidis and Ion Androutsopoulos. 2017. A Deep Learning Approach to Contract Element Extraction.. In JURIX. 155â\\x80\\x93164.\\n[11] Ilias Chalkidis, Ion Androutsopoulos, and Achilleas Michos. 2018. Obligation and prohibition extraction using hierarchical rnns. arXiv preprint arXiv:1805.03871 (2018).\\n[12] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, and Ion Androut- sopoulos. 2019. Large-Scale Multi-Label Text Classification on EU Legislation. arXiv preprint arXiv:1906.02192 (2019).\\n[13] Debajyoti Paul Chowdhury, Arghya Biswas, Tomasz Sosnowski, and Kristina Yordanova. 2020. Towards Evaluating Plan Generation Approaches with Instruc- tional Texts. arXiv preprint arXiv:2001.04186 (2020).\\n[14] Peter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, et al. 2019. Fromâ\\x80\\x99Fâ\\x80\\x99toâ\\x80\\x99Aâ\\x80\\x99on the NY Regents Science Exams: An Overview of the Aristo Project. arXiv preprint arXiv:1909.01958 (2019).\\n[15] Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, et al. 1996. Using the framework. Technical Report. Technical Report LRE 62-051 D-16, The FraCaS Consortium.\\n[16] Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recog- nising textual entailment challenge. In Machine Learning Challenges Workshop. Springer, 177â\\x80\\x93190.\\n[17] Donald Davidson. 1967. The logical forms of action sentences. The Logic of Decision and Action (1967).\\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161 (2019). [20] Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the web. Commun. ACM 51, 12 (2008), 68â\\x80\\x9374. [21] Edward A Feigenbaum. 1992. Expert systems: principles and practice. (1992). [22] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building Watson: An overview of the DeepQA project. AI magazine 31, 3 (2010), 59â\\x80\\x9379.\\n[23] Noah S Friedland, Paul G Allen, Gavin Matthews, Michael Witbrock, David Baxter, Jon Curtis, Blake Shepard, Pierluigi Miraglia, Jurgen Angele, Steffen Staab, et al. 2004. Project halo: Towards a digital aristotle. AI magazine 25, 4 (2004), 29â\\x80\\x9329. [24] Wachara Fungwacharakorn and Ken Satoh. 2018. Legal Debugging in Proposi- tional Legal Representation. In JSAI International Symposium on Artificial Intelli- gence. Springer, 146â\\x80\\x93159.\\n[25] Bryan A Gardner. 2019. Blackâ\\x80\\x99s Law Dictionary (11 ed.). [26] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL- PASCAL workshop on textual entailment and paraphrasing. Association for Com- putational Linguistics, 1â\\x80\\x939.\\n[27] David Gunning, Vinay K Chaudhri, Peter E Clark, Ken Barker, Shaw-Yi Chaw, Mark Greaves, Benjamin Grosof, Alice Leung, David D McDonald, Sunil Mishra, et al. 2010. Project Halo UpdateÃ¢Ä\\x82Å¤Progress Toward Digital Aristotle. AI Maga- zine 31, 3 (2010), 33â\\x80\\x9358.\\n[28] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entail- ment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.\\n[29] Herbert Lionel Adolphus Hart and Herbert Lionel Adolphus Hart. 2012. The concept of law. Oxford university press.\\n[30] Robert Hellawell. 1980. A computer program for legal planning and analysis: Taxation of stock redemptions. Columbia Law Review 80, 7 (1980), 1363â\\x80\\x931398.\\n[31] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).\\n[32] Imran Khan, Muhammad Sher, Javed I Khan, Syed M Saqlain, Anwar Ghani, Husnain A Naqvi, and Muhammad Usman Ashraf. 2016. Conversion of legal text\\nto a logical rules set from medical law using the medical relational model and the world rule model for a medical decision support system. In Informatics, Vol. 3. Multidisciplinary Digital Publishing Institute, 2.\\n[33] Mi-Young Kim, Juliano Rabelo, and Randy Goebel. 2019. Statute Law Informa- tion Retrieval and Entailment. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law. 283â\\x80\\x93289.\\n[34] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\\n[35] Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A Corpus for Automatic Summarization of US Legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, Hong Kong, China, 48â\\x80\\x9356. https://doi.org/10.18653/v1/D19-5406\\n[36] John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. (2001).\\n[37] Robert S Ledley and Lee B Lusted. 1959. Reasoning foundations of medical diagnosis. Science 130, 3366 (1959), 9â\\x80\\x9321.\\n[38] L Thorne McCarty. 1976. Reflections on TAXMAN: An experiment in artificial intelligence and legal reasoning. Harv. L. Rev. 90 (1976), 837.\\n[39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111â\\x80\\x933119.\\n[40] Randolph A Miller, Harry E Pople Jr, and Jack D Myers. 1982. Internist-I, an experimental computer-based diagnostic consultant for general internal medicine. New England Journal of Medicine 307, 8 (1982), 468â\\x80\\x93476.\\n[41] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Betteridge, Andrew Carlson, Bhanava Dalvi, Matt Gardner, Bryan Kisiel, et al. 2018. Never-ending learning. Commun. ACM 61, 5 (2018), 103â\\x80\\x93115. [42] Terence Parsons. 1990. Events in the Semantics of English. Vol. 334. MIT press\\nCambridge, MA.\\n[43] Walter G Popp and Bernhard Schlink. 1974. Judith, a computer program to advise lawyers in reasoning a case. Jurimetrics J. 15 (1974), 303.\\n[44] Juliano Rabelo, Mi-Young Kim, and Randy Goebel. 2019. Combining Similarity and Transformer Methods for Case Law Entailment. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law. 290â\\x80\\x93296.\\n[45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim- its of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).\\n[46] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Donâ\\x80\\x99t Know: Unanswerable Questions for SQuAD. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.\\n[47] Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. 2019. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives. arXiv preprint arXiv:1911.00841 (2019).\\n[48] Edwina L Rissland, Kevin D Ashley, and Ronald Prescott Loui. 2003. AI and Law: A fruitful synergy. Artificial Intelligence 150, 1-2 (2003), 1â\\x80\\x9315.\\n[49] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series.\\n[50] Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim RocktÃ¤schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading. arXiv preprint arXiv:1809.01494 (2018).\\n[51] Ken Satoh, Kento Asai, Takamune Kogawa, Masahiro Kubota, Megumi Naka- mura, Yoshiaki Nishigai, Kei Shirakawa, and Chiaki Takano. 2010. PROLEG: an implementation of the presupposed ultimate fact theory of Japanese civil code by PROLOG technology. In JSAI International Symposium on Artificial Intelligence. Springer, 153â\\x80\\x93164.\\n[52] Marek J. Sergot, Fariba Sadri, Robert A. Kowalski, Frank Kriwaczek, Peter Ham- mond, and H Terese Cory. 1986. The British Nationality Act as a logic program. Commun. ACM 29, 5 (1986), 370â\\x80\\x93386.\\n[53] David M Sherman. 1987. A Prolog model of the income tax act of Canada. In Proceedings of the 1st international conference on Artificial intelligence and law. 127â\\x80\\x93136.\\n[54] Edward H Shortliffe and Bruce G Buchanan. 1975. A model of inexact reasoning in medicine. Mathematical biosciences 23, 3-4 (1975), 351â\\x80\\x93379.\\n[55] Jerrold Soh, How Khang Lim, and Ian Ernst Chai. 2019. Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. Association for Computational Linguistics, Minneapolis, Minnesota.\\n[56] Anne vdL Gardner. 1983. The design of a legal analysis program. In AAAI-83. 114â\\x80\\x93118.\\n[57] Lai Dac Viet, Vu Trong Sinh, Nguyen Le Minh, and Ken Satoh. 2017. ConvAMR: Abstract meaning representation parsing for legal document. arXiv preprint arXiv:1711.06141 (2017).\\n[58] Thiemo WambsganÃ\\x9f and HansjÃ¶rg Fromm. 2019. Mining User-Generated Repair Instructions from Automotive Web Communities. In Proceedings of the 52nd Hawaii International Conference on System Sciences.\\n[59] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier bench- mark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. 3261â\\x80\\x933275.\\n[60] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Mer- riÃ«nboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 (2015). [61] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 2369â\\x80\\x932380. https://doi.org/10. 18653/v1/D18-1259\\n[62] Masaharu Yoshioka, Yoshinobu Kano, Naoki Kiyota, and Ken Satoh. 2018. Overview of japanese statute law retrieval and entailment task at coliee-2018. In Twelfth International Workshop on Juris-informatics (JURISIN 2018).\\n[63] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large- scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326 (2018).\\n[64] Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019. Broad- Coverage Semantic Parsing as Transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3786â\\x80\\x933798. https://doi.org/ 10.18653/v1/D19-1392\\n[65] Ziqi Zhang, Philip Webster, Victoria S Uren, Andrea Varga, and Fabio Ciravegna. 2012. Automatically Extracting Procedural Knowledge from Instructional Texts using Natural Language Processing.. In LREC, Vol. 2012. 520â\\x80\\x93527.\\n[66] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2019. JEC-QA: A Legal-Domain Question Answering Dataset. arXiv preprint arXiv:1911.12011 (2019).',\n",
       " 'references': {'id': '1502.03167'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see here structured data, as well as unstructured data (the content, and to a limited degree, authors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the candidate to create a pipeline to load this data into a database of their choice, ensuring the schema is optimized for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the database, to not spend TOO much time on making the datbase, I handled Database Normilzation only up to the First Normal Form (1NF). For projects needing more complicated and in-depth setup, we can explore 2NF, 3NF, ... 5NF or other normilization technique. Also decided to use sqlite for the database for the simplicity of creation in the demo. **They are all found in models.py**\n",
    "\n",
    "To optimize for querying, we handled the database with 1NF normalization, indexed frequent query columns, added constraints, and limited size of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6dae467c-7a8d-4662-9cc1-9ad5158b74e5',\n",
       " 'arxiv_id': '2401.04088',\n",
       " 'title': 'Mixtral of Experts',\n",
       " 'summary': 'We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\\nMixtral has the same architecture as Mistral 7B, with the difference that each\\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\\neach layer, a router network selects two experts to process the current state\\nand combine their outputs. Even though each token only sees two experts, the\\nselected experts can be different at each timestep. As a result, each token has\\naccess to 47B parameters, but only uses 13B active parameters during inference.\\nMixtral was trained with a context size of 32k tokens and it outperforms or\\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\\nmultilingual benchmarks. We also provide a model fine-tuned to follow\\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\\nthe base and instruct models are released under the Apache 2.0 license.',\n",
       " 'content': \"4 2 0 2\\nn a J 8 ] G L . s c [\\n1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a\\n# Mixtral of Experts\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed\\n\\nAbstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â\\x80\\x93 Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\x80\\x93 chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\\n# Introduction\\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â\\x80\\x9cexpertsâ\\x80\\x9d) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\\nMixture of Experts Layer i gating inputs af outputs router expert\\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ\\x80\\x99s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture.\\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence.\\nWe also present Mixtral 8x7B â\\x80\\x93 Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\x80\\x93 chat model on human evaluation benchmarks. Mixtral â\\x80\\x93 Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\x80\\x93 Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\\n# 2 Architectural details\\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value\\ndim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts\\n# 2.1 Sparse Mixture of Experts\\nWe present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ\\x80\\x99s output. i.e. given n expert networks {E0, Ei, ..., Enâ\\x88\\x921}, the output of the expert layer is given by: Table 1: Model architecture.\\n# j nâ\\x80\\x94\\nG(x)i Â· Ei(x). i=0\\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use\\nG(x) := Softmax(TopK(x Â· Wg)),\\nwhere (TopK(â\\x84\\x93))i := â\\x84\\x93i if â\\x84\\x93i is among the top-K coordinates of logits â\\x84\\x93 â\\x88\\x88 Rn and (TopK(â\\x84\\x93))i := â\\x88\\x92â\\x88\\x9e otherwise. The value of K â\\x80\\x93 the number of experts used per token â\\x80\\x93 is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n while keeping K fixed, one\\n# 1https://mistral.ai/news/mixtral-of-experts/\\n2\\n4096 32 128 14336 32 8 32768 32000 8 2\\ncan increase the modelâ\\x80\\x99s parameter count while keeping its computational cost effectively constant. This motivates a distinction between the modelâ\\x80\\x99s total parameter count (commonly referenced as the sparse parameter count), which grows with n, and the number of parameters used for processing an individual token (called the active parameter count), which grows with K up to n.\\nMoE layers can be run efficiently on single GPUs with high performance specialized kernels. For example, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large sparse matrix multiplications, significantly enhancing the execution speed and naturally handling cases where different experts get a variable number of tokens assigned to them. Moreover, the MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and through a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE layerâ\\x80\\x99s execution, tokens meant to be processed by a specific expert are routed to the corresponding GPU for processing, and the expertâ\\x80\\x99s output is returned to the original token location. Note that EP introduces challenges in load balancing, as it is essential to distribute the workload evenly across the GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\\nIn a Transformer model, the MoE layer is applied independently per token and replaces the feed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU architecture as the expert function Ei(x) and set K = 2. This means each token is routed to two SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input token x is computed as:\\nn-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\\n# 3 Results\\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\\nâ\\x80¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\\nWorld Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â\\x80¢ Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â\\x80¢ Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 â\\x80¢ Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â\\x80¢ Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\\n(3-5-shot, English multiple-choice questions only)\\n80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning â\\x80\\x98Comprehension AGI Eval Math â\\x80\\x98Accuracy (%)\\nFigure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or matches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\\n3\\nActive Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0% 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3% 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1% 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6% 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0% 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%\\nTable 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mixtral 8x7B. â\\x80\\x98Mixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â\\x80\\x94= Mistral Â° 20 â\\x80\\x94e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\\nDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ\\x80\\x99 efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.\\nComparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n4\\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\\n# Mixtral 8x7B\\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\\n# 3.1 Multilingual benchmarks\\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%\\nTable 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\\n# 3.2 Long range performance\\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\\n3.8 â\\x80\\x94 Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length\\nPasskey Performance ry 3.8 â\\x80\\x94 Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length\\nFigure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\\n5\\n# 3.3 Bias Benchmarks\\nTo identify possible flaws to be corrected by fine-tuning / preference modeling, we measure the base model performance on Bias Benchmark for QA (BBQ) [24] and Bias in Open-Ended Language Generation Dataset (BOLD) [10]. BBQ is a dataset of hand-written question sets that target attested social biases against nine differ- ent socially-relevant categories: age, dis- ability status, gender identity, nationality, physical appearance, race/ethnicity, religion, socio-economic status, sexual orientation. BOLD is a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains.\\nLlama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\\n# Instruction Fine-tuning\\nWe train Mixtral â\\x80\\x93 Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by Direct Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral â\\x80\\x93 Instruct reaches a score of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December 2023. Independent human evaluation conducted by LMSys is reported in Figure 63 and shows that Mixtral â\\x80\\x93 Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\\nvs Arena Elo rating 1 MT-bench (score) License 1243 9.32 Proprietary 1192 8.96 Proprietary 1158 9.18 Proprietary Glaude-4 1149 7.9 Proprietary Claude-2.0 1131 8.06 Proprietary 1121 eS) Apache 2.0 Glaude-2.4 1117 8.18 Proprietary GPT-3..5-Turbo-9613 1117 8.39 Proprietary Gemini..Pro 1141 Proprietary Glas ta 1110 7.85 Proprietary Tulu-2-0P0-708 1110 7.89 AI2 ImpACT Low-risk Yi-34B-Chat 1110 Yi License GPT-3.5:Turbo-0314 1105 7.94 Proprietary Llama-2-79b-chat 1077 6.86 Llama 2 Community\\nFigure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro (1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\\n3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\\n6\\n# 5 Routing analysis\\nIn this section, we perform a small analysis on the expert selection by the router. In particular, we are interested to see if during training some experts specialized to some specific domains (e.g. mathematics, biology, philosophy, etc.).\\nTo investigate this, we measure the distribution of selected experts on different subsets of The Pile validation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31 respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts), and for Philosophy (PhilPapers) documents.\\nOnly for DM Mathematics we note a marginally different distribution of experts. This divergence is likely a consequence of the datasetâ\\x80\\x99s synthetic nature and its limited coverage of the natural language spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very correlated to the input and output embeddings respectively.\\nThis suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows examples of text from different domains (Python code, mathematics, and English), where each token is highlighted with a background color corresponding to its selected expert. The figure shows that words such as â\\x80\\x98selfâ\\x80\\x99 in Python and â\\x80\\x98Questionâ\\x80\\x99 in English often get routed through the same expert even though they involve multiple tokens. Similarly, in code, the indentation tokens are always assigned to the same experts, particularly at the first and last layers where the hidden states are more correlated to the input and output of the model.\\nWe also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con- secutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\\n0.20 0.15 0.10 0.05 layer: 15 0.20 0.15 0.10 0.05 layer: 31 Selection proportion 0.20 0.15 0.10 0.05 Expert ID | | ArXiv | Github | | PhilPapers | StackExchange | | DM Mathematics | | Gutenberg | | PubMed Abstracts | | Wikipedia (en)\\nFigure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for layers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform sampling. Here, we consider experts that are either selected as a first or second choice by the router. A breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\\n7\\nLayer 0 First choice Layer 15 Layer 31 Layer 0 First or second choice Layer 15 Layer 31 ArXiv DM Mathematics Github Gutenberg PhilPapers PubMed Abstracts StackExchange Wikipedia (en) 14.0% 14.1% 14.9% 13.9% 13.6% 14.2% 13.6% 14.4% 27.9% 28.4% 28.1% 26.1% 25.3% 24.6% 27.2% 23.6% 22.7% 19.7% 19.7% 26.3% 22.1% 22.0% 23.6% 25.3% 46.5% 44.9% 49.9% 49.5% 46.9% 48.6% 48.2% 49.8% 62.3% 67.0% 66.9% 63.1% 61.9% 61.6% 64.6% 62.1% 52.9% 44.5% 49.2% 52.2% 51.3% 51.8% 53.6% 51.8%\\nTable 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is assigned to a token i and its following token i+1. We report whether the first chosen expert is the same, or whether the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion of repetitions in the case of random assignments is 1 5 7 â\\x89\\x88 46% for â\\x80\\x9cFirst and second choiceâ\\x80\\x9d. Repetitions at the first layer are close to random, but are significantly higher at layers 15 and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\\nconsecutive assignments is significantly higher than random for higher layers. This has implications in how one might optimize the model for fast training and inference. For example, cases with high locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism. Conversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\\n# 6 Conclusion\\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.\\nLayer 0 Layer 15 Layer 31 class MoeLayer(nn. Module) : â\\x80\\x9cinit__(self, experts//List [nn.Modutel,) | Super (V7 init assert len(experts) > 0 self. experts = nn.ModuleList((experts) self. gate = gate self.args = moe_args def forward(self, inputs: torch.Tensor): inputs _squashed = inputs. view(-1,_ inputs.| gate_logits = self.gatel inputs_squashed) weights, selected_experts = torch. topk( gate_logits, Self-args.nun_experts_Ã© weights! = nri.|funct ional softinax'( weights, din=1, dtype=torch. float, ).type_as|(inputs) results| = torch. zeros_ ike! linputs_squashe for i, expert in enunerate(self. experts): batch_idx,! nth_expert = torch. wnere( results [batch_idx] += weights [batch_i input s_squashed [batch_idx] ) return resutts:.view las{(inputs) class NoeLayer (nn. Module) = def _ init__(self, experts! List'{nri.Modulelly Super (Tz init_t assert len (experts) > 9) self.experts = nn. ModuleList((experits)) def forward(self, inputs: torch. Tensor)?! inputs_squashed = inputs.View(-1) inputs) gate_logits = self.gatel inputs_squashed) weights, selected_experts = torch. topk( getellogits, self.argssnun_experts pe weightsâ\\x80\\x99 = nn. functionallsoftmax(Â® Weights, dtypextorch. floaty ) type_as (inputs) results| = torch. zerdsillikel(input siiequashe| for i, expert in enumerate (self. experts): batch idx, nth_expert = torch.where(s results [batch_idx] += weights [batch_iÂ¢ inputs|_squashed[batch idx], y return resultsiiview jas (inputs) class| MoeLayer(nn. Module): def init__(self, expertsâ\\x80\\x99 List|fifi.Modulel) Super(Ve_init_O) assert len(experts) > 0 self, experts = nn.ModuleListl(@xperits)) self. gate = gate Self.args = moe_args def forward(self, inputs: torch. Tensor): inputs_squashed = inputs.view(=1, inputs) gate_logits = self.gate( inputs_squashed) weights, selected_experts = torch. topk( gate_logits, self.argssfum_experts_pe weights) nni.unct iorial.isoftinax( YP Yiitype_as (inputs) results = torch. zerosillikel(inputslisquashe| for i, expert in enunerate(self.experts): batch_idx, nth_expert = torch.where(s results [batch_idx] += weights [batch_iÂ¢ inputs_squashed [batch_idx] ) return) results\\\\iviewilas|(inputs)) Tuestiond] Solve â\\x80\\x94AINr 27K SLIT! and SORT, lanswers 4 Question?â\\x80\\x99 Calculate Baiasoazusaaly 4111270 iAnswer: -841469015.544 (Question! Letâ\\x80\\x99 x(gy = 94g # Hl Let! q(clJ= Zee #] IAnswer: S4ea - 30 â\\x80\\x98Question#! Solve Azer Â¥ 27HE = Ate and 1505 lanswer:) 4 Calculate ~eaieseiaz. saa Â¥ 417127. ~841469015.544 â\\x80\\x98Answer: (Questor â\\x80\\x98Answer: etâ\\x80\\x99 x(q) = 9*g Â¥ Wl Let! ql)! = 2eele Sara â\\x80\\x94 30 question Solve -42Â¥e1E B7eC= â\\x80\\x94Ad67 and 130%] answers \\\\questionÂ®| calculate savesona2.saq + auaz7. Answer: -847469015.544 â\\x80\\x98OÂ¥o)H A Let q(el = (questiond! Let! x(a) = awed | Answers 54a ~ â\\x80\\x98A model airplane flies stower when flying into tt jwind and faster with wind at its back. when Launcl Iright angles to the wind,â\\x80\\x9d cross wind,| its groun Icompared with! flying in still air is (A) the same (B) greater (C) less (0)! either! grea lor less dependingâ\\x80\\x99 on wind speed i nodelaitp ane) URE slover when flying into eH lind and faster with wind at its back. When) launch Tight angles to the wind, a cross wind,. its) grounc Compared with â\\x80\\x98lying in stitt air is (A) the same (18) greater) (C) less (D)! either grea lor less depending on wind speed H model airplane flies slower! when flying inte th wind and faster with wind at its backâ\\x80\\x99. When Launcl [right angles to the wind, a cross wind, its grounc Icompared with flying in still air is (A) the sane (B) greater (C) less (0)! either gree jor less depending on wind speed\\nFigure 8: Text samples where each token is colored with the first expert choice. The selection of experts appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\\n8\\n# Acknowledgements\\nWe thank the CoreWeave and Scaleway teams for technical support as we trained our models. We are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\\n# References\\n[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n[2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.\\n[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432â\\x80\\x937439, 2020.\\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n[5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018.\\n[6] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057â\\x80\\x934086. PMLR, 2022.\\n[7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\\n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â\\x80\\x93872, 2021.\\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.\\n[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Advances in Neural Information Processing Systems, 34:29335â\\x80\\x9329347, 2021.\\n9\\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\\n[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\n[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\\n[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, pages 453â\\x80\\x93466, 2019.\\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\\n[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\\n[23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.\\n[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp- son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193, 2021.\\n[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.\\n[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, pages 99â\\x80\\x93106, 2021.\\n[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com- monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n[29] Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques- tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å\\x81ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\\n[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\\n10\\n[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.\\n[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103â\\x80\\x937114, 2022.\\n11\\n# Either choice\\n0\\nLayer -- 0.3 0.2 0 Layer 0 -- First choice 0.3 Layer 0 -- Second choice 0.3 < 2 t Layer 15 -- First choice fe} Q 0.3 Â° a 0.2 el (el er rere! ie it len | ie} o 0 v Layer 15 -- Second choice 8 03 0.2 0 Layer 31 -- Either choice\\n# Expert ID\\nArXiv Github PhilPapers. StackExchange |_| | |_| | | DM Mathematics | Gutenberg || PubMed Abstracts | Wikipedia (en)\\nFigure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated by whether the expert was selected as first or second choice, or either. The â\\x80\\x9cEither choiceâ\\x80\\x9d case is equivalent to Figure 7. The gray dashed vertical line marks 1\\n12\\nFirst choice 9 w is) Â° N a Â° N is) Â° An wu 0.7 0.6 Proportion of repeated assignments 0.5 Layer source â\\x80\\x94e ArXiv â\\x80\\x94eâ\\x80\\x94 DM Mathematics â\\x80\\x94e Github â\\x80\\x94eâ\\x80\\x94 Gutenberg â\\x80\\x94eâ\\x80\\x94 PhilPapers â\\x80\\x94eâ\\x80\\x94 PubMed â\\x80\\x94e- StackExchange â\\x80\\x94e-â\\x80\\x94 Wikipedia (en)\\n# Abstracts\\nFigure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across datasets with less repetitions for DM Mathematics.\\n13\",\n",
       " 'source': 'http://arxiv.org/pdf/2401.04088',\n",
       " 'comment': 'See more details at https://mistral.ai/news/mixtral-of-experts/',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.LG',\n",
       " 'published': '20240108',\n",
       " 'updated': '20240108',\n",
       " 'total_tokens': None,\n",
       " 'chunk_count': None,\n",
       " 'authors': [{'id': '25d44dc7-9b8e-4298-9da8-8467c2e3abc8',\n",
       "   'name': 'Albert Q. Jiang',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.404568',\n",
       "   'date_modified': '2024-11-05T17:13:20.184716'},\n",
       "  {'id': '53d13ec0-4af2-4b6b-8a82-26278dee1219',\n",
       "   'name': 'Alexandre Sablayrolles',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.407968',\n",
       "   'date_modified': '2024-11-05T17:13:25.849252'},\n",
       "  {'id': '8cc9dad7-b3de-471a-82b5-14c8082e1533',\n",
       "   'name': 'Antoine Roux',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.408789',\n",
       "   'date_modified': '2024-11-05T17:13:09.428533'},\n",
       "  {'id': '81dfe41b-d3c1-4a4f-890d-975d30afd12e',\n",
       "   'name': 'Arthur Mensch',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.409584',\n",
       "   'date_modified': '2024-11-05T17:13:22.033784'},\n",
       "  {'id': 'dba13d36-8b0e-42f8-a996-882f5f32b994',\n",
       "   'name': 'Blanche Savary',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.410141',\n",
       "   'date_modified': '2024-11-05T17:13:09.428551'},\n",
       "  {'id': '825ad17e-9e07-446c-bfb3-ed44d8907a40',\n",
       "   'name': 'Chris Bamford',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.410600',\n",
       "   'date_modified': '2024-11-05T17:13:09.589844'},\n",
       "  {'id': 'fa759b93-2944-4ded-ad7a-9acd6ebe09c3',\n",
       "   'name': 'Devendra Singh Chaplot',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.411075',\n",
       "   'date_modified': '2024-11-05T17:13:23.030584'},\n",
       "  {'id': 'aa380cd7-27bf-4b79-a2be-d77917977c88',\n",
       "   'name': 'Diego de las Casas',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.411672',\n",
       "   'date_modified': '2024-11-05T17:13:21.443806'},\n",
       "  {'id': '48abbe2e-dd74-4cef-aabf-4ceae141dc4e',\n",
       "   'name': 'Emma Bou Hanna',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.412385',\n",
       "   'date_modified': '2024-11-05T17:13:09.428509'},\n",
       "  {'id': 'bac6f122-7515-4592-a254-6794f8091f21',\n",
       "   'name': 'Florian Bressand',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.412924',\n",
       "   'date_modified': '2024-11-05T17:13:09.589854'},\n",
       "  {'id': '13660bad-f077-47b7-87fb-94ba5639447a',\n",
       "   'name': 'Gianna Lengyel',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.413357',\n",
       "   'date_modified': '2024-11-05T17:13:09.589811'},\n",
       "  {'id': 'eaf3a35d-31e6-4318-8f03-8e74ab191f52',\n",
       "   'name': 'Guillaume Bour',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.413822',\n",
       "   'date_modified': '2024-11-05T17:13:09.428558'},\n",
       "  {'id': '33a6e891-d387-4813-ba00-4b647bf0f033',\n",
       "   'name': 'Guillaume Lample',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.414501',\n",
       "   'date_modified': '2024-11-05T17:13:33.656485'},\n",
       "  {'id': '580bc5f2-74b2-4156-9a2d-67853fa1e779',\n",
       "   'name': 'Lucile Saulnier',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.416118',\n",
       "   'date_modified': '2024-11-05T17:13:24.001451'},\n",
       "  {'id': '9321fabc-5d47-48fe-bc76-d46014135f5e',\n",
       "   'name': 'Lélio Renard Lavaud',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.415258',\n",
       "   'date_modified': '2024-11-05T17:13:09.589847'},\n",
       "  {'id': 'a919554e-52f1-4b66-8d95-b9450cbb2a99',\n",
       "   'name': 'Marie-Anne Lachaux',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.416823',\n",
       "   'date_modified': '2024-11-05T17:13:29.695785'},\n",
       "  {'id': 'd4b55bb6-440e-43e1-b265-810653fe5a9b',\n",
       "   'name': 'Pierre Stock',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.417729',\n",
       "   'date_modified': '2024-11-05T17:13:29.092402'},\n",
       "  {'id': '43de6d25-b4ec-4ff2-ad18-8ca93e02a3b0',\n",
       "   'name': 'Sandeep Subramanian',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.418328',\n",
       "   'date_modified': '2024-11-05T17:13:33.656488'},\n",
       "  {'id': '3ee02ca2-abfc-4db6-b7cd-8f7048717c75',\n",
       "   'name': 'Sophia Yang',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.418802',\n",
       "   'date_modified': '2024-11-05T17:13:09.428502'},\n",
       "  {'id': 'e34bf3d9-e0ef-45fd-9382-76d2aa46a8ad',\n",
       "   'name': 'Szymon Antoniak',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.419201',\n",
       "   'date_modified': '2024-11-05T17:13:09.428553'},\n",
       "  {'id': '1e3d24a0-90e8-41f5-8f65-122b25978908',\n",
       "   'name': 'Teven Le Scao',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.419886',\n",
       "   'date_modified': '2024-11-05T17:13:28.498525'},\n",
       "  {'id': '3dbd27ac-aaf9-4958-ad80-6eecc013b1e1',\n",
       "   'name': 'Thibaut Lavril',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.420876',\n",
       "   'date_modified': '2024-11-05T17:13:27.855576'},\n",
       "  {'id': '12ab8526-5781-4c87-9a6f-f5d14a07702f',\n",
       "   'name': 'Thomas Wang',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.421325',\n",
       "   'date_modified': '2024-11-05T17:13:22.848395'},\n",
       "  {'id': 'e36075ab-283d-45a3-8742-2a25cde6a142',\n",
       "   'name': 'Théophile Gervet',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.420396',\n",
       "   'date_modified': '2024-11-05T17:13:09.428556'},\n",
       "  {'id': '4400673b-8b11-46d5-9be1-35b3af88bf04',\n",
       "   'name': 'Timothée Lacroix',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.421776',\n",
       "   'date_modified': '2024-11-05T17:13:33.216100'},\n",
       "  {'id': 'c8a3d0b1-442f-46ce-8152-e474426ceaf9',\n",
       "   'name': 'William El Sayed',\n",
       "   'email': None,\n",
       "   'institution': None,\n",
       "   'date_created': '2024-11-05T17:13:09.422251',\n",
       "   'date_modified': '2024-11-05T17:13:09.589857'}],\n",
       " 'categories': [{'id': '44b4cc9e-ce6d-40df-8a3c-2402a0581996',\n",
       "   'code': 'cs.CL',\n",
       "   'name': 'cs.CL',\n",
       "   'description': None,\n",
       "   'date_created': '2024-11-05T17:13:09.429598',\n",
       "   'date_modified': '2024-11-05T17:13:34.061610'},\n",
       "  {'id': '1a9f9c6c-f9b0-40b2-b835-6591d51bee5a',\n",
       "   'code': 'cs.LG',\n",
       "   'name': 'cs.LG',\n",
       "   'description': None,\n",
       "   'date_created': '2024-11-05T17:13:09.424594',\n",
       "   'date_modified': '2024-11-05T17:13:34.072119'}],\n",
       " 'date_created': '2024-11-05T17:13:09.430005',\n",
       " 'date_modified': '2024-11-05T17:13:09.430013',\n",
       " 'citations': [],\n",
       " 'cited_by': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example:\n",
    "Session = get_or_create_database()\n",
    "    \n",
    "with Session() as session:\n",
    "    paper_obj = get_papers(session=session,limit=1)[0]\n",
    "    paper_dict = paper_obj.to_dict()\n",
    "\n",
    "paper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data may contain noise or require transformation (e.g., text cleaning, parsing nested JSON, handling missing values).\n",
    "\n",
    "### The candidate should demonstrate how they preprocess the data for efficient storage and later retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main player in data preprocessing is ArXivPreprocessor. It cleans up messy Unicode characters, removes unwanted special characters with regex, and converts raw academic papers into clean, standardized chunks. For storage efficiency, I've built in metadata tracking that includes token counts, document relationships (through pre/post chunk IDs), and citation references**\n",
    "\n",
    "**There are two speeds, 0 and 1. 0 is a slower, more expensive approach but uses much more advanced algorithms to chunk the data (StatisticalChunking) otherwise we use a simple RollingWindowSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bababako/side_projects/llm_projects/disney/RAG_Example_DSNY/preprocessor.py:47: UserWarning: Splitters are being deprecated. They have moved to their own package. Please migrate to the `semantic-chunkers` package. More information can be found at:\n",
      "https://github.com/aurelio-labs/semantic-chunkers\n",
      "  self.splitter = RollingWindowSplitter(\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArXivPreprocessor(encoder=encoder,speed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-05 18:58:34 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2401.04088 into 41 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 238\n",
      "  - Total Splits: 41\n",
      "  - Splits by Threshold: 34\n",
      "  - Splits by Max Chunk Size: 6\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 43\n",
      "  - Maximum Token Size of Split: 482\n",
      "  - Similarity Split Ratio: 0.83\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "with Session() as session:\n",
    "    chunks_data = arxiv.preprocess_paper(paper_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2401.04088#0',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks.',\n",
       "  'prechunk_id': '',\n",
       "  'postchunk_id': '2401.04088#1',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 0,\n",
       "  'token_count': 453,\n",
       "  'semantic_score': 0.3007558847087624},\n",
       " {'id': '2401.04088#1',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Both the base and instruct models are released under the Apache 2.0 license. Code: https: github.com mistralai mistral-src Webpage: https: mistral.ai news mixtral-of-experts Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks.',\n",
       "  'prechunk_id': '2401.04088#0',\n",
       "  'postchunk_id': '2401.04088#2',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 1,\n",
       "  'token_count': 289,\n",
       "  'semantic_score': 0.24161856435414136},\n",
       " {'id': '2401.04088#2',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'In particular, Mixture of Experts Layer i gating inputs af outputs router expert Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1).',\n",
       "  'prechunk_id': '2401.04088#1',\n",
       "  'postchunk_id': '2401.04088#3',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 2,\n",
       "  'token_count': 432,\n",
       "  'semantic_score': 0.2946814561774593},\n",
       " {'id': '2401.04088#3',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'The model architecture parameters are summarized in Table 1. Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by:',\n",
       "  'prechunk_id': '2401.04088#2',\n",
       "  'postchunk_id': '2401.04088#4',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 3,\n",
       "  'token_count': 142,\n",
       "  'semantic_score': 0.27385190250137603},\n",
       " {'id': '2401.04088#4',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Table 1: Model architecture. j nâ G(x)i Â Ei(x). i 0 Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use G(x) : Softmax(TopK(x Â Wg)), where (TopK(â ))i : â i if â i is among the top-K coordinates of logits â â Rn and (TopK(â ))i : â â otherwise. The value of K â the number of experts used per token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n while keeping K fixed, one 1https: mistral.ai news mixtral-of-experts 2 4096 32 128 14336 32 8 32768 32000 8 2 can increase the modelâ s parameter count while keeping its computational cost effectively constant. This motivates a distinction between the modelâ s total parameter count (commonly referenced as the sparse parameter count), which grows with n, and the number of parameters used for processing an individual token (called the active parameter count), which grows with K up to n.',\n",
       "  'prechunk_id': '2401.04088#3',\n",
       "  'postchunk_id': '2401.04088#5',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 4,\n",
       "  'token_count': 328,\n",
       "  'semantic_score': 0.3111182335559642},\n",
       " {'id': '2401.04088#5',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'MoE layers can be run efficiently on single GPUs with high performance specialized kernels. For example, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large sparse matrix multiplications, significantly enhancing the execution speed and naturally handling cases where different experts get a variable number of tokens assigned to them. Moreover, the MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and through a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE layerâ s execution, tokens meant to be processed by a specific expert are routed to the corresponding GPU for processing, and the expertâ s output is returned to the original token location. Note that EP introduces challenges in load balancing, as it is essential to distribute the workload evenly across the GPUs to prevent overloading individual GPUs or hitting computational bottlenecks. In a Transformer model, the MoE layer is applied independently per token and replaces the feed-forward (FFN) sub-block of the transformer block.',\n",
       "  'prechunk_id': '2401.04088#4',\n",
       "  'postchunk_id': '2401.04088#6',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 5,\n",
       "  'token_count': 212,\n",
       "  'semantic_score': 0.3012580006360325},\n",
       " {'id': '2401.04088#6',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'For Mixtral we use the same SwiGLU architecture as the expert function Ei(x) and set K 2. This means each token is routed to two SwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input token x is computed as: n-1 y Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i 0 This formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token. 3 Results We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison.',\n",
       "  'prechunk_id': '2401.04088#5',\n",
       "  'postchunk_id': '2401.04088#7',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 6,\n",
       "  'token_count': 176,\n",
       "  'semantic_score': 0.2752563542540831},\n",
       " {'id': '2401.04088#7',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'We measure performance on a wide variety of tasks categorized as follow: â Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30] World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â Math: GSM8K [9] (8-shot) with maj 8 and MATH [17] (4-shot) with maj 4 â Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34] (3-5-shot, English multiple-choice questions only) 80 SE Mistral 78 LLaMA27B Sl LLaMA134B, jam Mistral 78 LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning â Comprehension AGI Eval Math â Accuracy ( ) Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or matches Llama 2 70B on all benchmarks.',\n",
       "  'prechunk_id': '2401.04088#6',\n",
       "  'postchunk_id': '2401.04088#8',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 7,\n",
       "  'token_count': 376,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#8',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'In particular, it is vastly superior in mathematics and code generation. 3 Active Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K 7B 44.4 77.1 69.5 77.9 68.7 43.2 17.5 56.6 11.6 26.1 3.9 16.0 13B 55.6 80.7 72.9 80.8 75.2 48.8 16.7 64.0 18.9 35.4 6.0 34.3 33B 56.8 83.7 76.2 82.2 79.6 54.4 24.1 68.5 25.0 40.9 8.4 44.1 70B 69.9 85.4 80.4 82.6 79.9 56.5 25.4 73.0 29.3 49.8 13.8 69.6 7B 62.5 81.0 74.2 82.2 80.5 54.9 23.2 62.5 26.2 50.2 12.7 50.0 13B 70.6 84.4 77.2 83.6 83.1 59.7 30.6 71.5 40.2 60.7 28.4 74.4 Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference. 70 Mixtral 8x7B. â Mixtral 8x7B Mixtral 8x7B 355 o Es E60!',\n",
       "  'prechunk_id': '2401.04088#7',\n",
       "  'postchunk_id': '2401.04088#9',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 8,\n",
       "  'token_count': 430,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#9',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Mistral 78 2681 Mistral 78 3 3 s0 5 A 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 564 340 g al Mistral 78 ee Mistral 78 3 5 Â 30 5 eo â Mistral Â 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â 13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B 8x7B) vs Llama 2 (7B 13B 70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B 13B 70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.',\n",
       "  'prechunk_id': '2401.04088#8',\n",
       "  'postchunk_id': '2401.04088#10',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 9,\n",
       "  'token_count': 343,\n",
       "  'semantic_score': 0.25166638557032633},\n",
       " {'id': '2401.04088#10',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.',\n",
       "  'prechunk_id': '2401.04088#9',\n",
       "  'postchunk_id': '2401.04088#11',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 10,\n",
       "  'token_count': 217,\n",
       "  'semantic_score': 0.3079272396293445},\n",
       " {'id': '2401.04088#11',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B. 4 LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9 70.0 70.6 HellaSwag (10-shot) 87.1 85.5 86.7 ARC Challenge (25-shot) 85.1 85.2 85.8 WinoGrande (5-shot) 83.2 81.6 81.2 MBPP (pass 1) 49.8 52.2 60.7 GSM-8K (5-shot) 53.6 57.1 58.4 MT Bench (for Instruct Models) 6.86 8.32 8.30 Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.',\n",
       "  'prechunk_id': '2401.04088#10',\n",
       "  'postchunk_id': '2401.04088#12',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 11,\n",
       "  'token_count': 375,\n",
       "  'semantic_score': 0.2682920959686541},\n",
       " {'id': '2401.04088#12',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts. 3.1 Multilingual benchmarks Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4. Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9 65.4 49.0 39.3 68.1 49.9 49.9 72.5 64.3 49.4 70.9 65.1 58.2 77.4 70.9 54.3 73.0 71.5 55.4 77.6 72.5 52.8 75.1 70.9 41.1 63.3 48.7 47.3 68.7 64.2 45.7 69.8 52.3 50.5 74.5 66.0 Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian. 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100 retrieval accuracy regardless of the context length or the position of passkey in the sequence.',\n",
       "  'prechunk_id': '2401.04088#11',\n",
       "  'postchunk_id': '2401.04088#13',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 12,\n",
       "  'token_count': 482,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#13',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc 3.8 â Mixtral_8x7B 3.5 32 3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 0.6 3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100 retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases. 5 3.3 Bias Benchmarks To identify possible flaws to be corrected by fine-tuning preference modeling, we measure the base model performance on Bias Benchmark for QA (BBQ) [24] and Bias in Open-Ended Language Generation Dataset (BOLD) [10].',\n",
       "  'prechunk_id': '2401.04088#12',\n",
       "  'postchunk_id': '2401.04088#14',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 13,\n",
       "  'token_count': 370,\n",
       "  'semantic_score': 0.30427799404490374},\n",
       " {'id': '2401.04088#14',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'BBQ is a dataset of hand-written question sets that target attested social biases against nine differ- ent socially-relevant categories: age, dis- ability status, gender identity, nationality, physical appearance, race ethnicity, religion, socio-economic status, sexual orientation. BOLD is a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains. Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5 56.0 BOLD sentiment score (avg Â std) gender profession religious_ideology political_ideology race 0.293 Â 0.073 0.218 Â 0.073 0.188 Â 0.133 0.149 Â 0.140 0.232 Â 0.049 0.323 Â 0.045 0.243 Â 0.087 0.144 Â 0.089 0.186 Â 0.146 0.232 Â 0.052 Figure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD). We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0 vs 51.5 ). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.',\n",
       "  'prechunk_id': '2401.04088#13',\n",
       "  'postchunk_id': '2401.04088#15',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 14,\n",
       "  'token_count': 369,\n",
       "  'semantic_score': 0.25997832666448206},\n",
       " {'id': '2401.04088#15',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Instruction Fine-tuning We train Mixtral â Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by Direct Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral â Instruct reaches a score of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December 2023. Independent human evaluation conducted by LMSys is reported in Figure 63 and shows that Mixtral â Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat. vs Arena Elo rating 1 MT-bench (score) License 1243 9.32 Proprietary 1192 8.96 Proprietary 1158 9.18 Proprietary Glaude-4 1149 7.9 Proprietary Claude-2.0 1131 8.06 Proprietary 1121 eS) Apache 2.0 Glaude-2.4 1117 8.18 Proprietary GPT-3..5-Turbo-9613 1117 8.39 Proprietary Gemini..Pro 1141 Proprietary Glas ta 1110 7.85 Proprietary Tulu-2-0P0-708 1110 7.89 AI2 ImpACT Low-risk Yi-34B-Chat 1110 Yi License GPT-3.5:Turbo-0314 1105 7.94 Proprietary Llama-2-79b-chat 1077 6.86 Llama 2 Community Figure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro (1111), and Llama-2-70b-chat (1077).',\n",
       "  'prechunk_id': '2401.04088#14',\n",
       "  'postchunk_id': '2401.04088#16',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 15,\n",
       "  'token_count': 452,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#16',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Mixtral is currently the best open-weights model by a large margin. 3https: huggingface.co spaces lmsys chatbot-arena-leaderboard 6 5 Routing analysis In this section, we perform a small analysis on the expert selection by the router. In particular, we are interested to see if during training some experts specialized to some specific domains (e.g. mathematics, biology, philosophy, etc.). To investigate this, we measure the distribution of selected experts on different subsets of The Pile validation dataset [14].',\n",
       "  'prechunk_id': '2401.04088#15',\n",
       "  'postchunk_id': '2401.04088#17',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 16,\n",
       "  'token_count': 111,\n",
       "  'semantic_score': 0.3188886164415702},\n",
       " {'id': '2401.04088#17',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31 respectively being the first and the last layers of the model). Surprisingly, we do not observe obvious patterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts), and for Philosophy (PhilPapers) documents. Only for DM Mathematics we note a marginally different distribution of experts.',\n",
       "  'prechunk_id': '2401.04088#16',\n",
       "  'postchunk_id': '2401.04088#18',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 17,\n",
       "  'token_count': 114,\n",
       "  'semantic_score': 0.30721495836371215},\n",
       " {'id': '2401.04088#18',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'This divergence is likely a consequence of the datasetâ s synthetic nature and its limited coverage of the natural language spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very correlated to the input and output embeddings respectively. This suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows examples of text from different domains (Python code, mathematics, and English), where each token is highlighted with a background color corresponding to its selected expert. The figure shows that words such as â selfâ in Python and â Questionâ in English often get routed through the same expert even though they involve multiple tokens. Similarly, in code, the indentation tokens are always assigned to the same experts, particularly at the first and last layers where the hidden states are more correlated to the input and output of the model. We also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we observe some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con- secutive tokens that get the same expert assignments per domain and layer. The proportion of repeated 0.20 0.15 0.10 0.05 layer: 15 0.20 0.15 0.10 0.05 layer: 31 Selection proportion 0.20 0.15 0.10 0.05 Expert ID ArXiv Github PhilPapers StackExchange DM Mathematics Gutenberg PubMed Abstracts Wikipedia (en) Figure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for layers 0, 15, and 31. The gray dashed vertical line marks 1 8, i.e. the proportion expected with uniform sampling. Here, we consider experts that are either selected as a first or second choice by the router.',\n",
       "  'prechunk_id': '2401.04088#17',\n",
       "  'postchunk_id': '2401.04088#19',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 18,\n",
       "  'token_count': 378,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#19',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'A breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix. 7 Layer 0 First choice Layer 15 Layer 31 Layer 0 First or second choice Layer 15 Layer 31 ArXiv DM Mathematics Github Gutenberg PhilPapers PubMed Abstracts StackExchange Wikipedia (en) 14.0 14.1 14.9 13.9 13.6 14.2 13.6 14.4 27.9 28.4 28.1 26.1 25.3 24.6 27.2 23.6 22.7 19.7 19.7 26.3 22.1 22.0 23.6 25.3 46.5 44.9 49.9 49.5 46.9 48.6 48.2 49.8 62.3 67.0 66.9 63.1 61.9 61.6 64.6 62.1 52.9 44.5 49.2 52.2 51.3 51.8 53.6 51.8 Table 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is assigned to a token i and its following token i 1. We report whether the first chosen expert is the same, or whether the same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion of repetitions in the case of random assignments is 1 5 7 â 46 for â First and second choiceâ . Repetitions at the first layer are close to random, but are significantly higher at layers 15 and 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers. consecutive assignments is significantly higher than random for higher layers.',\n",
       "  'prechunk_id': '2401.04088#18',\n",
       "  'postchunk_id': '2401.04088#20',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 19,\n",
       "  'token_count': 403,\n",
       "  'semantic_score': 0.31529117046305016},\n",
       " {'id': '2401.04088#20',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'This has implications in how one might optimize the model for fast training and inference. For example, cases with high locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism. Conversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix. 6 Conclusion In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B).',\n",
       "  'prechunk_id': '2401.04088#19',\n",
       "  'postchunk_id': '2401.04088#21',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 20,\n",
       "  'token_count': 208,\n",
       "  'semantic_score': 0.23470953414034207},\n",
       " {'id': '2401.04088#21',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': \"We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains. Layer 0 Layer 15 Layer 31 class MoeLayer(nn. Module) : â init__(self, experts List [nn.Modutel,) Super (V7 init assert len(experts) 0 self. experts nn.ModuleList((experts) self. gate gate self.args moe_args def forward(self, inputs: torch.Tensor): inputs _squashed inputs. view(-1,_ inputs. gate_logits self.gatel inputs_squashed) weights, selected_experts torch. topk( gate_logits, Self-args.nun_experts_Ã weights! nri. funct ional softinax'( weights, din 1, dtype torch. float, ).type_as (inputs) results torch. zeros_ ike! linputs_squashe for i, expert in enunerate(self. experts): batch_idx,! nth_expert torch. wnere( results [batch_idx] weights [batch_i input s_squashed [batch_idx] ) return resutts:.view las{(inputs) class NoeLayer (nn. Module) def _ init__(self, experts! List'{nri.Modulelly Super (Tz init_t assert len (experts) 9) self.experts nn. ModuleList((experits)) def forward(self, inputs: torch. Tensor)?! inputs_squashed inputs.View(-1) inputs) gate_logits self.gatel inputs_squashed) weights, selected_experts torch. topk( getellogits, self.argssnun_experts pe weightsâ nn. functionallsoftmax(Â Weights, dtypextorch. floaty ) type_as (inputs) results torch. zerdsillikel(input siiequashe for i, expert in enumerate (self. experts): batch idx, nth_expert torch.where(s results [batch_idx] weights [batch_iÂ inputs _squashed[batch idx], y return resultsiiview jas (inputs) class MoeLayer(nn.\",\n",
       "  'prechunk_id': '2401.04088#20',\n",
       "  'postchunk_id': '2401.04088#22',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 21,\n",
       "  'token_count': 463,\n",
       "  'semantic_score': None},\n",
       " {'id': '2401.04088#22',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Module): def init__(self, expertsâ List fifi.Modulel) Super(Ve_init_O) assert len(experts) 0 self, experts nn.ModuleListl( xperits)) self. gate gate Self.args moe_args def forward(self, inputs: torch. Tensor): inputs_squashed inputs.view( 1, inputs) gate_logits self.gate( inputs_squashed) weights, selected_experts torch. topk( gate_logits, self.argssfum_experts_pe weights) nni.unct iorial.isoftinax( YP Yiitype_as (inputs) results torch. zerosillikel(inputslisquashe for i, expert in enunerate(self.experts): batch_idx, nth_expert torch.where(s results [batch_idx] weights [batch_iÂ inputs_squashed [batch_idx] ) return) results iviewilas (inputs)) Tuestiond] Solve â AINr 27K SLIT! and SORT, lanswers 4 Question?â Calculate Baiasoazusaaly 4111270 iAnswer: -841469015.544 (Question!',\n",
       "  'prechunk_id': '2401.04088#21',\n",
       "  'postchunk_id': '2401.04088#23',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 22,\n",
       "  'token_count': 233,\n",
       "  'semantic_score': 0.2821580776822831},\n",
       " {'id': '2401.04088#23',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Letâ x(gy 94g Hl Let! q(clJ Zee ] IAnswer: S4ea - 30 â Question ! Solve Azer Â 27HE Ate and 1505 lanswer:) 4 Calculate eaieseiaz. saa Â 417127. 841469015.544 â Answer: (Questor â Answer: etâ x(q) 9 g Â Wl Let! ql)! 2eele Sara â 30 question Solve -42Â e1E B7eC â Ad67 and 130 ] answers questionÂ calculate savesona2.saq auaz7. Answer: -847469015.544 â OÂ o)H A Let q(el (questiond!',\n",
       "  'prechunk_id': '2401.04088#22',\n",
       "  'postchunk_id': '2401.04088#24',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 23,\n",
       "  'token_count': 153,\n",
       "  'semantic_score': 0.311621111000343},\n",
       " {'id': '2401.04088#24',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Let! x(a) awed Answers 54a â A model airplane flies stower when flying into tt jwind and faster with wind at its back. when Launcl Iright angles to the wind,â cross wind, its groun Icompared with! flying in still air is (A) the same (B) greater (C) less (0)! either! grea lor less dependingâ on wind speed i nodelaitp ane) URE slover when flying into eH lind and faster with wind at its back. When) launch Tight angles to the wind, a cross wind,. its) grounc Compared with â lying in stitt air is (A) the same (18) greater) (C) less (D)! either grea lor less depending on wind speed H model airplane flies slower! when flying inte th wind and faster with wind at its backâ . When Launcl [right angles to the wind, a cross wind, its grounc Icompared with flying in still air is (A) the sane (B) greater (C) less (0)! either gree jor less depending on wind speed Figure 8:',\n",
       "  'prechunk_id': '2401.04088#23',\n",
       "  'postchunk_id': '2401.04088#25',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 24,\n",
       "  'token_count': 243,\n",
       "  'semantic_score': 0.10199214131271296},\n",
       " {'id': '2401.04088#25',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Text samples where each token is colored with the first expert choice. The selection of experts appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers. 8 Acknowledgements We thank the CoreWeave and Scaleway teams for technical support as we trained our models. We are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working alongside us to make a sparse mixture of experts compatible with TensorRT-LLM. References [1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.',\n",
       "  'prechunk_id': '2401.04088#24',\n",
       "  'postchunk_id': '2401.04088#26',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 25,\n",
       "  'token_count': 225,\n",
       "  'semantic_score': 0.1884052735960702},\n",
       " {'id': '2401.04088#26',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. [3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432â 7439, 2020. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer.',\n",
       "  'prechunk_id': '2401.04088#25',\n",
       "  'postchunk_id': '2401.04088#27',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 26,\n",
       "  'token_count': 218,\n",
       "  'semantic_score': 0.15902157825551422},\n",
       " {'id': '2401.04088#27',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018. [6] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057â 4086.',\n",
       "  'prechunk_id': '2401.04088#26',\n",
       "  'postchunk_id': '2401.04088#28',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 27,\n",
       "  'token_count': 104,\n",
       "  'semantic_score': 0.29724586447448653},\n",
       " {'id': '2401.04088#28',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'PMLR, 2022. [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes no questions. arXiv preprint arXiv:1905.10044, 2019. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.',\n",
       "  'prechunk_id': '2401.04088#27',\n",
       "  'postchunk_id': '2401.04088#29',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 28,\n",
       "  'token_count': 268,\n",
       "  'semantic_score': 0.15186279676736056},\n",
       " {'id': '2401.04088#29',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â 872, 2021. [11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023. [12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022. [13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia.',\n",
       "  'prechunk_id': '2401.04088#28',\n",
       "  'postchunk_id': '2401.04088#30',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 29,\n",
       "  'token_count': 154,\n",
       "  'semantic_score': 0.23613127398388156},\n",
       " {'id': '2401.04088#30',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022. [14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi.',\n",
       "  'prechunk_id': '2401.04088#29',\n",
       "  'postchunk_id': '2401.04088#31',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 30,\n",
       "  'token_count': 164,\n",
       "  'semantic_score': 0.21496445089230967},\n",
       " {'id': '2401.04088#31',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Advances in Neural Information Processing Systems, 34:29335â 29347, 2021. 9 [16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.',\n",
       "  'prechunk_id': '2401.04088#30',\n",
       "  'postchunk_id': '2401.04088#32',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 31,\n",
       "  'token_count': 279,\n",
       "  'semantic_score': 0.21969388662314898},\n",
       " {'id': '2401.04088#32',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, pages 453â 466, 2019. [21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.',\n",
       "  'prechunk_id': '2401.04088#31',\n",
       "  'postchunk_id': '2401.04088#33',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 32,\n",
       "  'token_count': 168,\n",
       "  'semantic_score': 0.15064731227098493},\n",
       " {'id': '2401.04088#33',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Gshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [23] Amirkeivan Mohtashami and Martin Jaggi.',\n",
       "  'prechunk_id': '2401.04088#32',\n",
       "  'postchunk_id': '2401.04088#34',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 33,\n",
       "  'token_count': 114,\n",
       "  'semantic_score': 0.16335575846760977},\n",
       " {'id': '2401.04088#34',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp- son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193, 2021. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. [26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.',\n",
       "  'prechunk_id': '2401.04088#33',\n",
       "  'postchunk_id': '2401.04088#35',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 34,\n",
       "  'token_count': 197,\n",
       "  'semantic_score': 0.10478955981133835},\n",
       " {'id': '2401.04088#35',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, pages 99â 106, 2021. [27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com- monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.',\n",
       "  'prechunk_id': '2401.04088#34',\n",
       "  'postchunk_id': '2401.04088#36',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 35,\n",
       "  'token_count': 129,\n",
       "  'semantic_score': 0.24146405155298092},\n",
       " {'id': '2401.04088#36',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [29] Mirac Suzgun, Nathan Scales, Nathanael SchÃ rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.',\n",
       "  'prechunk_id': '2401.04088#35',\n",
       "  'postchunk_id': '2401.04088#37',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 36,\n",
       "  'token_count': 156,\n",
       "  'semantic_score': 0.29169848109856317},\n",
       " {'id': '2401.04088#37',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Commonsenseqa: A ques- tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.',\n",
       "  'prechunk_id': '2401.04088#36',\n",
       "  'postchunk_id': '2401.04088#38',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 37,\n",
       "  'token_count': 132,\n",
       "  'semantic_score': 0.12349744787484294},\n",
       " {'id': '2401.04088#38',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. 10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.',\n",
       "  'prechunk_id': '2401.04088#37',\n",
       "  'postchunk_id': '2401.04088#39',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 38,\n",
       "  'token_count': 173,\n",
       "  'semantic_score': 0.1900095520413141},\n",
       " {'id': '2401.04088#39',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103â 7114, 2022. 11 Either choice 0 Layer -- 0.3 0.2 0 Layer 0 -- First choice 0.3 Layer 0 -- Second choice 0.3 2 t Layer 15 -- First choice fe} Q 0.3 Â a 0.2 el (el er rere! ie it len ie} o 0 v Layer 15 -- Second choice 8 03 0.2 0 Layer 31 -- Either choice Expert ID ArXiv Github PhilPapers. StackExchange _ _ DM Mathematics Gutenberg PubMed Abstracts Wikipedia (en) Figure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated by whether the expert was selected as first or second choice, or either. The â Either choiceâ case is equivalent to Figure 7. The gray dashed vertical line marks 1 12 First choice 9 w is) Â N a Â N is) Â An wu 0.7 0.6 Proportion of repeated assignments 0.5 Layer source â e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- StackExchange â e-â Wikipedia (en) Abstracts Figure 10:',\n",
       "  'prechunk_id': '2401.04088#38',\n",
       "  'postchunk_id': '2401.04088#40',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 39,\n",
       "  'token_count': 369,\n",
       "  'semantic_score': 0.3198734503264267},\n",
       " {'id': '2401.04088#40',\n",
       "  'title': 'Mixtral of Experts',\n",
       "  'content': 'Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across datasets with less repetitions for DM Mathematics. 13',\n",
       "  'prechunk_id': '2401.04088#39',\n",
       "  'postchunk_id': '',\n",
       "  'arxiv_id': '2401.04088',\n",
       "  'references': [],\n",
       "  'chunk_index': 40,\n",
       "  'token_count': 43,\n",
       "  'semantic_score': None}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorization:\n",
    "\n",
    "##### 1. Using a pre-trained language model or embeddings model, ask the candidate to convert the unstructured text into embeddings.\n",
    "##### 2. Store these embeddings in a vector storage solution of their choice, ensuring the pipeline can handle batch processing for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I've built a Vectorizer class that takes any embedding model and converts our preprocessed chunks into vectors, storing them in Pinecone with all our metadata intact and linked. Everything's built to handle scale - it processes in configurable batches using a batch_generator, has error handling, and shows real-time progress bars. Plus, storing vectors with their full context makes retrieval super smart later!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(encoder=encoder,index=index,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 1/9 [00:01<00:15,  1.93s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 2/9 [00:03<00:10,  1.55s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 3/9 [00:04<00:07,  1.24s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 4/9 [00:05<00:06,  1.21s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▌    | 5/9 [00:06<00:04,  1.16s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 6/9 [00:07<00:03,  1.09s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 7/9 [00:08<00:02,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 8/9 [00:08<00:00,  1.07it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 9/9 [00:09<00:00,  1.04s/it]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 41\n",
      "- Failed chunks: 0\n",
      "- Processing time: 9.45 seconds\n",
      "- Rate: 4.34 chunks/second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_chunks': 41,\n",
       " 'processed_chunks': 41,\n",
       " 'failed_chunks': 0,\n",
       " 'processing_time': 9.452132,\n",
       " 'chunks_per_second': 4.337645729027059}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "vectorizer.vectorize_and_store(metadata_list=chunks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query and Retrieve:\n",
    "\n",
    "##### 1. Create a simple API or script that allows querying based on a given text prompt.The query should retrieve similar embeddings from the vector store and return the corresponding records from the database.\n",
    "\n",
    "##### 2. Include a use case for Retriever-Augmented Generation (RAG), where the retrieved data is used to generate a summary or response based on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There was a very simple way of avoiding the use of agents and custom vectorization by using langchain retrievers and vectorstores. Taking this approach would've let to a faster rag conversation. But I wanted to demonstrate my ability of making custom agents, and leaning towards my understanding of custom tools and control of the RAG environment. Albeit, this is still a extremely simple example of agent creation, and if needed for more complex demonstrations reach out to me through eneadodi.com** \n",
    "\n",
    "**Finally, because we have a database with very important information related to summary, citations, authors, etc we can use agents for things like:**\n",
    "* finding documents of authors given a document used in response\n",
    "* finding documents that cite the current document used in response\n",
    "* finding and querying by category for comparison\n",
    "\n",
    "**These can all be done by simply adding more tools on the tools list below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool \n",
    "def get_rag_context(query:str)->list[dict]:\n",
    "    \"\"\"\n",
    "    Helper Tool Function that should always be called for an input by the user to get context.\n",
    "    \"\"\"\n",
    "    return vectorizer.query(query)\n",
    "\n",
    "tools = [get_rag_context]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a helpful AI assistant with access to relevant context through the get_rag_context tool. \n",
    "        Always use this tool first to retrieve context before answering questions.\n",
    "        After getting context, use it to provide accurate and informed responses.\n",
    "        If the context doesn't contain relevant information, acknowledge that and provide a general response.\n",
    "        The context is largely in relation to science publications, so you know how to summarize those as an expert.\n",
    "        Get it right and you get $100 tip!\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_rag_context` with `{'query': 'algorithms used to find the results of the risk'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[{'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'ly, AC LL and RF-C LL for the combinations of AC and RF-C with LL. Character Error Rate AC LL 17.24 17.81 17.31 18.4 35.89 38.12 37.0 40.87 L 10, Î 0.3 L 30, Î 0.3 L 10, Î 0.5 L 30, Î 0.5 RF-C AC LL RF-C LL 17.82 18.16 35.84 37.6 16.65 17.1 34.6 36.36 16.97 17.47 35 36.6 Table 2: Our IWSLT 2014 machine translation results with a convolutional encoder compared to the previous work by Ranzato et al. Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work. The training data comprises about 153,000 German-English sentence pairs. In addition we considered a larger WMT14 English-French dataset Cho et al. (2014) with more than 12 million examples. For further information about the data we refer the reader to Appendix B. The return is deï ned as a smoothed and rescaled version of the BLEU score. Speciï cally, we start all n-gram counts from 1 instead of ', 'score': 0.353827566, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 17.0, 'content': 'Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work.', 'postchunk_id': '1607.07086#18', 'prechunk_id': '1607.07086#16', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 246.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'm the pre-trained actor, while the actorâ s parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2. 4 RELATED WORK In other recent RL-inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results. Another recently proposed method is to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure itself (Wiseman Rush, 2016). This method follows the more general strategy called â learning as search optimizationâ (DaumÂ e III Marcu, 2005). This is an interesting alternative to our approach; however, it is designed speciï cally for the precise ', 'score': 0.350776404, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 12.0, 'content': 'However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results.', 'postchunk_id': '1607.07086#13', 'prechunk_id': '1607.07086#11', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 481.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'Please see Table 1 for an explanation of abbreviations. The asterisk identiï es results from (Wiseman Rush, 2016). Model greedy search beam search LL 22.53 23.87 BSO 23.83 25.48 LL 25.82 27.56 RF-C RF-C LL 27.42 27.75 27.7 28.3 AC 27.27 27.75 AC LL 27.49 28.53 Table 4: Our WMT 14 machine translation results compared to the previous work. Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset. In addition, we performed an ablation study. We found that using a target network was crucial; while the joint actor-critic training was still progressing with Î3Î 0.1, with Î3Î 1.0 it did not work at all. Similarly important was the value penalty described in Equation (10). We found that good values of the Î coefï cient were in the range [10â 3, 10â 6]. Other techniques, such as reward shaping an', 'score': 0.346584976, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 21.0, 'content': 'Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset.', 'postchunk_id': '1607.07086#22', 'prechunk_id': '1607.07086#20', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 286.0}}]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe context retrieved does not provide specific information about algorithms used to find the results of risk in a particular study. However, it does mention various algorithms related to sequence prediction and machine translation, such as:\n",
      "\n",
      "1. **Actor-Critic Algorithm**: This is a reinforcement learning algorithm used for sequence prediction, which combines the benefits of both actor and critic networks.\n",
      "\n",
      "2. **REINFORCE Algorithm**: A policy gradient method used in reinforcement learning, known for its high variance.\n",
      "\n",
      "3. **SARSA and OLPOMDP**: Standard value-based reinforcement learning algorithms applied to structured prediction.\n",
      "\n",
      "4. **Imitation Learning Algorithms**: Such as SEARN and DAGGER, which rely on an expert policy to provide action sequences for the policy to imitate.\n",
      "\n",
      "5. **Minimum (Bayes) Risk Training**: An approach that replaces the domain over which the task score expectation is defined with a small subset, typically an n-best list or a sample.\n",
      "\n",
      "If you have a specific study or context in mind regarding risk assessment, please provide more details so I can assist you better.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=True)\n",
    "output = agent.query(\"what algorithms were used to find the results of the risk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FULL EXAMPLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vectorizer:Found 10 papers to process\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]INFO:vectorizer:Starting preprocessing of 5 papers...\n",
      "\u001b[32m2024-11-05 19:37:54 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 1607.07086 into 49 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 438\n",
      "  - Total Splits: 49\n",
      "  - Splits by Threshold: 31\n",
      "  - Splits by Max Chunk Size: 17\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 102\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 49\n",
      "- Failed chunks: 0\n",
      "- Processing time: 10.68 seconds\n",
      "- Rate: 4.59 chunks/second\n",
      "\u001b[32m2024-11-05 19:38:06 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2210.03050 into 252 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 3314\n",
      "  - Total Splits: 252\n",
      "  - Splits by Threshold: 185\n",
      "  - Splits by Max Chunk Size: 66\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 100\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 51/51 [00:44<00:00,  1.15it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 252\n",
      "- Failed chunks: 0\n",
      "- Processing time: 44.52 seconds\n",
      "- Rate: 5.66 chunks/second\n",
      "\u001b[32m2024-11-05 19:38:58 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2302.00763 into 43 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 724\n",
      "  - Total Splits: 43\n",
      "  - Splits by Threshold: 34\n",
      "  - Splits by Max Chunk Size: 8\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 100\n",
      "  - Maximum Token Size of Split: 496\n",
      "  - Similarity Split Ratio: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 9/9 [00:08<00:00,  1.12it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 43\n",
      "- Failed chunks: 0\n",
      "- Processing time: 8.03 seconds\n",
      "- Rate: 5.35 chunks/second\n",
      "\u001b[32m2024-11-05 19:39:08 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 1609.07061 into 68 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 622\n",
      "  - Total Splits: 68\n",
      "  - Splits by Threshold: 46\n",
      "  - Splits by Max Chunk Size: 21\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 88\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 14/14 [00:11<00:00,  1.20it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 68\n",
      "- Failed chunks: 0\n",
      "- Processing time: 11.68 seconds\n",
      "- Rate: 5.82 chunks/second\n",
      "\u001b[32m2024-11-05 19:39:22 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2304.01904 into 83 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 925\n",
      "  - Total Splits: 83\n",
      "  - Splits by Threshold: 64\n",
      "  - Splits by Max Chunk Size: 18\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 17\n",
      "  - Maximum Token Size of Split: 494\n",
      "  - Similarity Split Ratio: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.11it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 83\n",
      "- Failed chunks: 0\n",
      "- Processing time: 15.26 seconds\n",
      "- Rate: 5.44 chunks/second\n",
      "INFO:vectorizer:Processed and committed 5/5 papers...\n",
      "INFO:vectorizer:\n",
      "    Processing completed:\n",
      "    - Total papers: 5\n",
      "    - Successfully processed: 5\n",
      "    - Failed to process: 0\n",
      "    - Time taken: 107.22 seconds\n",
      "    - Average rate: 0.05 papers/second\n",
      "    \n",
      "Processed: 5, Failed: 0:  50%|█████     | 5/10 [01:47<01:47, 21.56s/it]INFO:vectorizer:Starting preprocessing of 5 papers...\n",
      "\u001b[32m2024-11-05 19:39:41 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2302.05442 into 105 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 936\n",
      "  - Total Splits: 105\n",
      "  - Splits by Threshold: 68\n",
      "  - Splits by Max Chunk Size: 36\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 37\n",
      "  - Maximum Token Size of Split: 630\n",
      "  - Similarity Split Ratio: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 21/21 [00:17<00:00,  1.22it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 105\n",
      "- Failed chunks: 0\n",
      "- Processing time: 17.18 seconds\n",
      "- Rate: 6.11 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:00 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2301.10226 into 109 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 1067\n",
      "  - Total Splits: 109\n",
      "  - Splits by Threshold: 86\n",
      "  - Splits by Max Chunk Size: 22\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 101\n",
      "  - Maximum Token Size of Split: 669\n",
      "  - Similarity Split Ratio: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.20it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 109\n",
      "- Failed chunks: 0\n",
      "- Processing time: 18.28 seconds\n",
      "- Rate: 5.96 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:22 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2208.02294 into 58 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 724\n",
      "  - Total Splits: 58\n",
      "  - Splits by Threshold: 44\n",
      "  - Splits by Max Chunk Size: 13\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 69\n",
      "  - Maximum Token Size of Split: 499\n",
      "  - Similarity Split Ratio: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 12/12 [00:10<00:00,  1.16it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 58\n",
      "- Failed chunks: 0\n",
      "- Processing time: 10.37 seconds\n",
      "- Rate: 5.59 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:35 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2305.10142 into 51 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 388\n",
      "  - Total Splits: 51\n",
      "  - Splits by Threshold: 39\n",
      "  - Splits by Max Chunk Size: 11\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 101\n",
      "  - Maximum Token Size of Split: 498\n",
      "  - Similarity Split Ratio: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.33it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 51\n",
      "- Failed chunks: 0\n",
      "- Processing time: 8.30 seconds\n",
      "- Rate: 6.14 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:45 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2211.05102 into 76 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 556\n",
      "  - Total Splits: 76\n",
      "  - Splits by Threshold: 57\n",
      "  - Splits by Max Chunk Size: 18\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 10\n",
      "  - Maximum Token Size of Split: 498\n",
      "  - Similarity Split Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.21it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 76\n",
      "- Failed chunks: 0\n",
      "- Processing time: 13.27 seconds\n",
      "- Rate: 5.73 chunks/second\n",
      "INFO:vectorizer:Processed and committed 5/5 papers...\n",
      "INFO:vectorizer:\n",
      "    Processing completed:\n",
      "    - Total papers: 5\n",
      "    - Successfully processed: 5\n",
      "    - Failed to process: 0\n",
      "    - Time taken: 78.62 seconds\n",
      "    - Average rate: 0.06 papers/second\n",
      "    \n",
      "Processed: 10, Failed: 0: 100%|██████████| 10/10 [03:06<00:00, 18.66s/it]\n",
      "INFO:vectorizer:\n",
      "            Processing completed:\n",
      "            - Total papers: 10\n",
      "            - Successfully processed: 10\n",
      "            - Failed to process: 0\n",
      "            - Time taken: 186.60 seconds\n",
      "            - Average rate: 0.05 papers/second\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "#to load DIRECTLY from the database.\n",
    "stats= process_existing_papers(preprocessor=arxiv,vectorizer=vectorizer,db_url='sqlite:///arxiv_papers.db',stop_at=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_papers': 10, 'processed_papers': 10, 'failed_papers': 0, 'processing_time': 186.601471, 'papers_per_second': 0.053590145599656074}\n"
     ]
    }
   ],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "🤖The context does not provide specific information about the total cost of the project or study. However, it does mention that annotators were paid $1 per 10 pairs of natural language explanations (NLEs) they annotated. This gives some insight into the cost structure related to human annotation, but the overall cost would depend on the total number of annotations and other project expenses not detailed in the document.\n",
      "\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "🤖The study \"Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup\" achieved several key results:\n",
      "\n",
      "1. **Transfer Learning Success**: The study demonstrated that transferring natural language explanations (NLEs) from a parent task with abundant NLEs to a child task with few NLEs but abundant labels is effective. This approach helps in generating NLEs for the child task.\n",
      "\n",
      "2. **Methodology**: Four few-shot transfer learning methods were established, covering different fine-tuning combinations of labels and NLEs for both parent and child tasks. These methods were applied to transfer explainability from a large natural language inference dataset (e-SNLI) to two child tasks: pronoun resolution (using the small-e-WinoGrande dataset) and commonsense validation (ComVE).\n",
      "\n",
      "3. **Best Practices**: The study identified the best methods for this setup, indicating that the parent task significantly aids in NLE generation for the child tasks.\n",
      "\n",
      "Overall, the results highlight the potential of few-shot out-of-domain transfer learning in scenarios where the child task has limited NLEs but abundant labels, providing a practical solution to the challenge of acquiring task-specific NLEs.\n",
      "\n",
      "----------------------------\n",
      "👋 Catch you later!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"🔍 Ask me anything (or 'exit()' to quit): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit()':\n",
    "        print(\"👋 Catch you later!\")\n",
    "        agent.clear_chat_history\n",
    "        break\n",
    "        \n",
    "    try:\n",
    "        output = agent.query(user_input)\n",
    "        print(\"----------------------------\\n🤖\"+output+\"\\n\\n----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Oops! Something went wrong: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Documentation:\n",
    "\n",
    "##### a. The candidate should document their code, the thought process behind their design choices, and any trade-offs they considered (e.g., schema design, vector storage approach, etc.).\n",
    "\n",
    "\n",
    "**Documentation, including explanations of design choices, and code descriptions are found on the jupyter notebook and the files.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "# 6. Bonus:\n",
    "\n",
    "#####    Implement monitoring or logging for the data pipeline to track the data flow and identify potential bottlenecks.\n",
    "#####    Optimize the pipeline for scalability, such as handling larger files or parallel processing.\n",
    "\n",
    "\n",
    "**logging is implemented in all key aspects of preprocessing, vectorization, and data flow. Pipeline is optimized for scalability by handling work through batches where we handle offsets in a way that allows parallelization. Of course more work needs to be done to really use the batch handling correctly, and turning it into async functions as well. But for this very simple demonstration, I decided to leave out that work. If needed, reach out to me on eneadodi.com to ask for more complicated examples** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
