{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import local modules\n",
    "from data_loading import get_or_create_database,load_dataset,load_paper_data,get_paper,get_papers\n",
    "from preprocessor import ArXivPreprocessor\n",
    "from vectorizer import Vectorizer,process_existing_papers,process_papers_batch\n",
    "from rag import RAGAgent\n",
    "\n",
    "\n",
    "#import third part modules\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import tool\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "#import standard modules\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/bababako/side_projects/llm_projects/venv_rag/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "# initialize connection to pinecone\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Pinecone API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
    "\n",
    "#initialize encoder, pc, and spec. get dimension of the encoder as well\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"  # us-east-1\n",
    ")\n",
    "\n",
    "dims = len(encoder([\"test\"])[0])\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 41}},\n",
       " 'total_vector_count': 41}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#initialize the index. \n",
    "index_name = \"dsny-rag-quick\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # dimensionality of embed 3\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a dataset (e.g., JSON, CSV, or unstructured text files) that includes a mix of structured and unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\").shuffle(seed=42).select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see here structured data, as well as unstructured data (the content, and to a limited degree, authors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the candidate to create a pipeline to load this data into a database of their choice, ensuring the schema is optimized for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the database, to not spend TOO much time on making the datbase, I handled Database Normilzation only up to the First Normal Form (1NF). For projects needing more complicated and in-depth setup, we can explore 2NF, 3NF, ... 5NF or other normilization technique. Also decided to use sqlite for the database for the simplicity of creation in the demo. **They are all found in models.py**\n",
    "\n",
    "To optimize for querying, we handled the database with 1NF normalization, indexed frequent query columns, added constraints, and limited size of records.\n",
    "\n",
    "Finally I decided to keep the full 'content' on the Paper in case there is a use case of needing the exact document returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example:\n",
    "Session = get_or_create_database()\n",
    "    \n",
    "with Session() as session:\n",
    "    paper_obj = get_papers(session=session,limit=1)[0]\n",
    "    paper_dict = paper_obj.to_dict()\n",
    "\n",
    "paper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data may contain noise or require transformation (e.g., text cleaning, parsing nested JSON, handling missing values).\n",
    "\n",
    "### The candidate should demonstrate how they preprocess the data for efficient storage and later retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main player in data preprocessing is ArXivPreprocessor. It cleans up messy Unicode characters, removes unwanted special characters with regex, and converts raw academic papers into clean, standardized chunks. For storage efficiency, I've built in metadata tracking that includes token counts, document relationships (through pre/post chunk IDs), and citation references**\n",
    "\n",
    "**There are two speeds, 0 and 1. 0 is a slower, more expensive approach but uses much more advanced algorithms to chunk the data (StatisticalChunking) otherwise we use a simple RollingWindowSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bababako/side_projects/llm_projects/disney/RAG_Example_DSNY/preprocessor.py:47: UserWarning: Splitters are being deprecated. They have moved to their own package. Please migrate to the `semantic-chunkers` package. More information can be found at:\n",
      "https://github.com/aurelio-labs/semantic-chunkers\n",
      "  self.splitter = RollingWindowSplitter(\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArXivPreprocessor(encoder=encoder,speed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-05 18:58:34 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2401.04088 into 41 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 238\n",
      "  - Total Splits: 41\n",
      "  - Splits by Threshold: 34\n",
      "  - Splits by Max Chunk Size: 6\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 43\n",
      "  - Maximum Token Size of Split: 482\n",
      "  - Similarity Split Ratio: 0.83\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "with Session() as session:\n",
    "    chunks_data = arxiv.preprocess_paper(paper_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorization:\n",
    "\n",
    "##### 1. Using a pre-trained language model or embeddings model, ask the candidate to convert the unstructured text into embeddings.\n",
    "##### 2. Store these embeddings in a vector storage solution of their choice, ensuring the pipeline can handle batch processing for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I've built a Vectorizer class that takes any embedding model and converts our preprocessed chunks into vectors, storing them in Pinecone with all our metadata intact and linked. Everything's built to handle scale - it processes in configurable batches using a batch_generator, has error handling, and shows real-time progress bars. Plus, storing vectors with their full context makes retrieval super smart later!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(encoder=encoder,index=index,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 1/9 [00:01<00:15,  1.93s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 2/9 [00:03<00:10,  1.55s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 3/9 [00:04<00:07,  1.24s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 4/9 [00:05<00:06,  1.21s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▌    | 5/9 [00:06<00:04,  1.16s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 6/9 [00:07<00:03,  1.09s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 7/9 [00:08<00:02,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 8/9 [00:08<00:00,  1.07it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 9/9 [00:09<00:00,  1.04s/it]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 41\n",
      "- Failed chunks: 0\n",
      "- Processing time: 9.45 seconds\n",
      "- Rate: 4.34 chunks/second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_chunks': 41,\n",
       " 'processed_chunks': 41,\n",
       " 'failed_chunks': 0,\n",
       " 'processing_time': 9.452132,\n",
       " 'chunks_per_second': 4.337645729027059}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "vectorizer.vectorize_and_store(metadata_list=chunks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query and Retrieve:\n",
    "\n",
    "##### 1. Create a simple API or script that allows querying based on a given text prompt.The query should retrieve similar embeddings from the vector store and return the corresponding records from the database.\n",
    "\n",
    "##### 2. Include a use case for Retriever-Augmented Generation (RAG), where the retrieved data is used to generate a summary or response based on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There was a very simple way of avoiding the use of agents and custom vectorization by using langchain retrievers and vectorstores. Taking this approach would've let to a faster rag conversation. But I wanted to demonstrate my ability of making custom agents, and leaning towards my understanding of custom tools and control of the RAG environment. Albeit, this is still a extremely simple example of agent creation, and if needed for more complex demonstrations reach out to me through eneadodi.com** \n",
    "\n",
    "**Finally, because we have a database with very important information related to summary, citations, authors, etc we can use agents for things like:**\n",
    "* finding documents of authors given a document used in response\n",
    "* finding documents that cite the current document used in response\n",
    "* finding and querying by category for comparison\n",
    "\n",
    "**These can all be done by simply adding more tools on the tools list below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool \n",
    "def get_rag_context(query:str)->list[dict]:\n",
    "    \"\"\"\n",
    "    Helper Tool Function that should always be called for an input by the user to get context.\n",
    "    \"\"\"\n",
    "    return vectorizer.query(query)\n",
    "\n",
    "tools = [get_rag_context]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a helpful AI assistant with access to relevant context through the get_rag_context tool. \n",
    "        Always use this tool first to retrieve context before answering questions.\n",
    "        After getting context, use it to provide accurate and informed responses.\n",
    "        If the context doesn't contain relevant information, acknowledge that and provide a general response.\n",
    "        The context is largely in relation to science publications, so you know how to summarize those as an expert.\n",
    "        Get it right and you get $100 tip!\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_rag_context` with `{'query': 'algorithms used to find the results of the risk'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[{'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'ly, AC LL and RF-C LL for the combinations of AC and RF-C with LL. Character Error Rate AC LL 17.24 17.81 17.31 18.4 35.89 38.12 37.0 40.87 L 10, Î 0.3 L 30, Î 0.3 L 10, Î 0.5 L 30, Î 0.5 RF-C AC LL RF-C LL 17.82 18.16 35.84 37.6 16.65 17.1 34.6 36.36 16.97 17.47 35 36.6 Table 2: Our IWSLT 2014 machine translation results with a convolutional encoder compared to the previous work by Ranzato et al. Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work. The training data comprises about 153,000 German-English sentence pairs. In addition we considered a larger WMT14 English-French dataset Cho et al. (2014) with more than 12 million examples. For further information about the data we refer the reader to Appendix B. The return is deï ned as a smoothed and rescaled version of the BLEU score. Speciï cally, we start all n-gram counts from 1 instead of ', 'score': 0.353827566, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 17.0, 'content': 'Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work.', 'postchunk_id': '1607.07086#18', 'prechunk_id': '1607.07086#16', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 246.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'm the pre-trained actor, while the actorâ s parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2. 4 RELATED WORK In other recent RL-inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results. Another recently proposed method is to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure itself (Wiseman Rush, 2016). This method follows the more general strategy called â learning as search optimizationâ (DaumÂ e III Marcu, 2005). This is an interesting alternative to our approach; however, it is designed speciï cally for the precise ', 'score': 0.350776404, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 12.0, 'content': 'However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results.', 'postchunk_id': '1607.07086#13', 'prechunk_id': '1607.07086#11', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 481.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'Please see Table 1 for an explanation of abbreviations. The asterisk identiï es results from (Wiseman Rush, 2016). Model greedy search beam search LL 22.53 23.87 BSO 23.83 25.48 LL 25.82 27.56 RF-C RF-C LL 27.42 27.75 27.7 28.3 AC 27.27 27.75 AC LL 27.49 28.53 Table 4: Our WMT 14 machine translation results compared to the previous work. Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset. In addition, we performed an ablation study. We found that using a target network was crucial; while the joint actor-critic training was still progressing with Î3Î 0.1, with Î3Î 1.0 it did not work at all. Similarly important was the value penalty described in Equation (10). We found that good values of the Î coefï cient were in the range [10â 3, 10â 6]. Other techniques, such as reward shaping an', 'score': 0.346584976, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 21.0, 'content': 'Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset.', 'postchunk_id': '1607.07086#22', 'prechunk_id': '1607.07086#20', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 286.0}}]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe context retrieved does not provide specific information about algorithms used to find the results of risk in a particular study. However, it does mention various algorithms related to sequence prediction and machine translation, such as:\n",
      "\n",
      "1. **Actor-Critic Algorithm**: This is a reinforcement learning algorithm used for sequence prediction, which combines the benefits of both actor and critic networks.\n",
      "\n",
      "2. **REINFORCE Algorithm**: A policy gradient method used in reinforcement learning, known for its high variance.\n",
      "\n",
      "3. **SARSA and OLPOMDP**: Standard value-based reinforcement learning algorithms applied to structured prediction.\n",
      "\n",
      "4. **Imitation Learning Algorithms**: Such as SEARN and DAGGER, which rely on an expert policy to provide action sequences for the policy to imitate.\n",
      "\n",
      "5. **Minimum (Bayes) Risk Training**: An approach that replaces the domain over which the task score expectation is defined with a small subset, typically an n-best list or a sample.\n",
      "\n",
      "If you have a specific study or context in mind regarding risk assessment, please provide more details so I can assist you better.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=True)\n",
    "output = agent.query(\"what algorithms were used to find the results of the risk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FULL EXAMPLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vectorizer:Found 10 papers to process\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]INFO:vectorizer:Starting preprocessing of 5 papers...\n",
      "\u001b[32m2024-11-05 19:37:54 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 1607.07086 into 49 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 438\n",
      "  - Total Splits: 49\n",
      "  - Splits by Threshold: 31\n",
      "  - Splits by Max Chunk Size: 17\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 102\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 49\n",
      "- Failed chunks: 0\n",
      "- Processing time: 10.68 seconds\n",
      "- Rate: 4.59 chunks/second\n",
      "\u001b[32m2024-11-05 19:38:06 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2210.03050 into 252 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 3314\n",
      "  - Total Splits: 252\n",
      "  - Splits by Threshold: 185\n",
      "  - Splits by Max Chunk Size: 66\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 100\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 51/51 [00:44<00:00,  1.15it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 252\n",
      "- Failed chunks: 0\n",
      "- Processing time: 44.52 seconds\n",
      "- Rate: 5.66 chunks/second\n",
      "\u001b[32m2024-11-05 19:38:58 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2302.00763 into 43 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 724\n",
      "  - Total Splits: 43\n",
      "  - Splits by Threshold: 34\n",
      "  - Splits by Max Chunk Size: 8\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 100\n",
      "  - Maximum Token Size of Split: 496\n",
      "  - Similarity Split Ratio: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 9/9 [00:08<00:00,  1.12it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 43\n",
      "- Failed chunks: 0\n",
      "- Processing time: 8.03 seconds\n",
      "- Rate: 5.35 chunks/second\n",
      "\u001b[32m2024-11-05 19:39:08 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 1609.07061 into 68 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 622\n",
      "  - Total Splits: 68\n",
      "  - Splits by Threshold: 46\n",
      "  - Splits by Max Chunk Size: 21\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 88\n",
      "  - Maximum Token Size of Split: 500\n",
      "  - Similarity Split Ratio: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 14/14 [00:11<00:00,  1.20it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 68\n",
      "- Failed chunks: 0\n",
      "- Processing time: 11.68 seconds\n",
      "- Rate: 5.82 chunks/second\n",
      "\u001b[32m2024-11-05 19:39:22 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2304.01904 into 83 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 925\n",
      "  - Total Splits: 83\n",
      "  - Splits by Threshold: 64\n",
      "  - Splits by Max Chunk Size: 18\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 17\n",
      "  - Maximum Token Size of Split: 494\n",
      "  - Similarity Split Ratio: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 17/17 [00:15<00:00,  1.11it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 83\n",
      "- Failed chunks: 0\n",
      "- Processing time: 15.26 seconds\n",
      "- Rate: 5.44 chunks/second\n",
      "INFO:vectorizer:Processed and committed 5/5 papers...\n",
      "INFO:vectorizer:\n",
      "    Processing completed:\n",
      "    - Total papers: 5\n",
      "    - Successfully processed: 5\n",
      "    - Failed to process: 0\n",
      "    - Time taken: 107.22 seconds\n",
      "    - Average rate: 0.05 papers/second\n",
      "    \n",
      "Processed: 5, Failed: 0:  50%|█████     | 5/10 [01:47<01:47, 21.56s/it]INFO:vectorizer:Starting preprocessing of 5 papers...\n",
      "\u001b[32m2024-11-05 19:39:41 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2302.05442 into 105 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 936\n",
      "  - Total Splits: 105\n",
      "  - Splits by Threshold: 68\n",
      "  - Splits by Max Chunk Size: 36\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 37\n",
      "  - Maximum Token Size of Split: 630\n",
      "  - Similarity Split Ratio: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 21/21 [00:17<00:00,  1.22it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 105\n",
      "- Failed chunks: 0\n",
      "- Processing time: 17.18 seconds\n",
      "- Rate: 6.11 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:00 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2301.10226 into 109 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 1067\n",
      "  - Total Splits: 109\n",
      "  - Splits by Threshold: 86\n",
      "  - Splits by Max Chunk Size: 22\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 101\n",
      "  - Maximum Token Size of Split: 669\n",
      "  - Similarity Split Ratio: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 22/22 [00:18<00:00,  1.20it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 109\n",
      "- Failed chunks: 0\n",
      "- Processing time: 18.28 seconds\n",
      "- Rate: 5.96 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:22 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2208.02294 into 58 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 724\n",
      "  - Total Splits: 58\n",
      "  - Splits by Threshold: 44\n",
      "  - Splits by Max Chunk Size: 13\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 69\n",
      "  - Maximum Token Size of Split: 499\n",
      "  - Similarity Split Ratio: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 12/12 [00:10<00:00,  1.16it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 58\n",
      "- Failed chunks: 0\n",
      "- Processing time: 10.37 seconds\n",
      "- Rate: 5.59 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:35 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2305.10142 into 51 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 388\n",
      "  - Total Splits: 51\n",
      "  - Splits by Threshold: 39\n",
      "  - Splits by Max Chunk Size: 11\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 101\n",
      "  - Maximum Token Size of Split: 498\n",
      "  - Similarity Split Ratio: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.33it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 51\n",
      "- Failed chunks: 0\n",
      "- Processing time: 8.30 seconds\n",
      "- Rate: 6.14 chunks/second\n",
      "\u001b[32m2024-11-05 19:40:45 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2211.05102 into 76 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 556\n",
      "  - Total Splits: 76\n",
      "  - Splits by Threshold: 57\n",
      "  - Splits by Max Chunk Size: 18\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 10\n",
      "  - Maximum Token Size of Split: 498\n",
      "  - Similarity Split Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.21it/s]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 76\n",
      "- Failed chunks: 0\n",
      "- Processing time: 13.27 seconds\n",
      "- Rate: 5.73 chunks/second\n",
      "INFO:vectorizer:Processed and committed 5/5 papers...\n",
      "INFO:vectorizer:\n",
      "    Processing completed:\n",
      "    - Total papers: 5\n",
      "    - Successfully processed: 5\n",
      "    - Failed to process: 0\n",
      "    - Time taken: 78.62 seconds\n",
      "    - Average rate: 0.06 papers/second\n",
      "    \n",
      "Processed: 10, Failed: 0: 100%|██████████| 10/10 [03:06<00:00, 18.66s/it]\n",
      "INFO:vectorizer:\n",
      "            Processing completed:\n",
      "            - Total papers: 10\n",
      "            - Successfully processed: 10\n",
      "            - Failed to process: 0\n",
      "            - Time taken: 186.60 seconds\n",
      "            - Average rate: 0.05 papers/second\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "#to load DIRECTLY from the database.\n",
    "stats= process_existing_papers(preprocessor=arxiv,vectorizer=vectorizer,db_url='sqlite:///arxiv_papers.db',stop_at=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_papers': 10, 'processed_papers': 10, 'failed_papers': 0, 'processing_time': 186.601471, 'papers_per_second': 0.053590145599656074}\n"
     ]
    }
   ],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "🤖The results from algorithms used for abnormality detection, particularly in the context of out-of-distribution (OOD) detection and sequence prediction, include:\n",
      "\n",
      "1. **Out-of-Distribution Detection**: The Maximum Softmax Probability (MSP) method was used to classify whether a test point belongs to an in-distribution dataset (e.g., ImageNet) or an out-of-distribution dataset (e.g., Places365). The performance of this binary classification task was evaluated using metrics such as the Area Under the Receiver Operating Characteristic (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). These metrics help assess the model's ability to distinguish between in-distribution and out-of-distribution data points.\n",
      "\n",
      "2. **Sequence Prediction**: In the context of sequence prediction, experiments were conducted to recover strings of natural text from corrupted versions, a task referred to as spelling correction. This task demonstrated the model's ability to handle synthetic data without overfitting concerns. Additionally, experiments in automatic machine translation were performed using different models and datasets, showcasing the algorithm's applicability in real-world tasks.\n",
      "\n",
      "These results highlight the effectiveness of the algorithms in detecting abnormalities and handling sequence prediction tasks, providing valuable insights into their performance and potential applications.\n",
      "\n",
      "----------------------------\n",
      "👋 Catch you later!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"🔍 Ask me anything (or 'exit()' to quit): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit()':\n",
    "        print(\"👋 Catch you later!\")\n",
    "        agent.clear_chat_history\n",
    "        break\n",
    "        \n",
    "    try:\n",
    "        output = agent.query(user_input)\n",
    "        print(\"----------------------------\\n\")\n",
    "        print(\"🤖\"+output)\n",
    "        print(\"\\n----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Oops! Something went wrong: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Documentation:\n",
    "\n",
    "##### a. The candidate should document their code, the thought process behind their design choices, and any trade-offs they considered (e.g., schema design, vector storage approach, etc.).\n",
    "\n",
    "\n",
    "**Documentation, including explanations of design choices, and code descriptions are found on the jupyter notebook and the files.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "# 6. Bonus:\n",
    "\n",
    "#####    Implement monitoring or logging for the data pipeline to track the data flow and identify potential bottlenecks.\n",
    "#####    Optimize the pipeline for scalability, such as handling larger files or parallel processing.\n",
    "\n",
    "\n",
    "**logging is implemented in all key aspects of preprocessing, vectorization, and data flow. Pipeline is optimized for scalability by handling work through batches where we handle offsets in a way that allows parallelization. Of course more work needs to be done to really use the batch handling correctly, and turning it into async functions as well. But for this very simple demonstration, I decided to leave out that work. If needed, reach out to me on eneadodi.com to ask for more complicated examples** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
