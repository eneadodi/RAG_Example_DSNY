{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import local modules\n",
    "from data_loading import get_or_create_database,load_dataset_todb,load_paper_data,get_paper,get_papers\n",
    "from preprocessor import ArXivPreprocessor\n",
    "from vectorizer import Vectorizer,process_existing_papers,process_papers_batch\n",
    "from rag import RAGAgent\n",
    "\n",
    "\n",
    "#import third part modules\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from datasets import load_dataset\n",
    "from langchain.agents import tool\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "#import standard modules\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/bababako/side_projects/llm_projects/venv_rag/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "# initialize connection to pinecone\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Pinecone API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
    "\n",
    "#initialize encoder, pc, and spec. get dimension of the encoder as well\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"  # us-east-1\n",
    ")\n",
    "\n",
    "dims = len(encoder([\"test\"])[0])\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 976}},\n",
       " 'total_vector_count': 976}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#initialize the index. \n",
    "index_name = \"dsny-rag-quick\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # dimensionality of embed 3\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a dataset (e.g., JSON, CSV, or unstructured text files) that includes a mix of structured and unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\").shuffle(seed=42).select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2005.05257',\n",
       " 'title': 'A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering',\n",
       " 'summary': 'Legislation can be viewed as a body of prescriptive rules expressed in\\nnatural language. The application of legislation to facts of a case we refer to\\nas statutory reasoning, where those facts are also expressed in natural\\nlanguage. Computational statutory reasoning is distinct from most existing work\\nin machine reading, in that much of the information needed for deciding a case\\nis declared exactly once (a law), while the information needed in much of\\nmachine reading tends to be learned through distributional language statistics.\\nTo investigate the performance of natural language understanding approaches on\\nstatutory reasoning, we introduce a dataset, together with a legal-domain text\\ncorpus. Straightforward application of machine reading models exhibits low\\nout-of-the-box performance on our questions, whether or not they have been\\nfine-tuned to the legal domain. We contrast this with a hand-constructed\\nProlog-based system, designed to fully solve the task. These experiments\\nsupport a discussion of the challenges facing statutory reasoning moving\\nforward, which we argue is an interesting real-world task that can motivate the\\ndevelopment of models able to utilize prescriptive rules specified in natural\\nlanguage.',\n",
       " 'source': 'http://arxiv.org/pdf/2005.05257',\n",
       " 'authors': 'Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme',\n",
       " 'categories': 'cs.CL',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20200511',\n",
       " 'updated': '20200812',\n",
       " 'content': '0 2 0 2\\ng u A 2 1 ] L C . s c [\\n3 v 7 5 2 5 0 . 5 0 0 2 : v i X r a\\n# A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering\\nNils Holzenberger Johns Hopkins University Baltimore, Maryland, USA nilsh@jhu.edu\\nAndrew Blair-Stanek U. of Maryland Carey School of Law Baltimore, Maryland, USA Johns Hopkins University Baltimore, Maryland, USA ablair-stanek@law.umaryland.edu\\nBenjamin Van Durme Johns Hopkins University Baltimore, Maryland, USA vandurme@cs.jhu.edu\\nABSTRACT Legislation can be viewed as a body of prescriptive rules expressed in natural language. The application of legislation to facts of a case we refer to as statutory reasoning, where those facts are also ex- pressed in natural language. Computational statutory reasoning is distinct from most existing work in machine reading, in that much of the information needed for deciding a case is declared exactly once (a law), while the information needed in much of machine read- ing tends to be learned through distributional language statistics. To investigate the performance of natural language understanding approaches on statutory reasoning, we introduce a dataset, together with a legal-domain text corpus. Straightforward application of ma- chine reading models exhibits low out-of-the-box performance on our questions, whether or not they have been fine-tuned to the legal domain. We contrast this with a hand-constructed Prolog-based system, designed to fully solve the task. These experiments support a discussion of the challenges facing statutory reasoning moving forward, which we argue is an interesting real-world task that can motivate the development of models able to utilize prescriptive rules specified in natural language.\\nCCS CONCEPTS â\\x80¢ Applied computing â\\x86\\x92 Law; â\\x80¢ Computing methodologies â\\x86\\x92 Natural language processing; Knowledge representation and reason- ing.\\n# KEYWORDS Law, NLP, Reasoning, Prolog\\nsources, e.g. textbooks, and connecting high-performance symbolic solvers with large-scale language models.\\nIn parallel, models have begun to consider task definitions like Machine Reading (MR) [46] and Recognizing Textual Entailment (RTE) [15, 16] as not requiring the use of explicit structure. Instead, the problem is cast as one of mapping inputs to high-dimensional, dense representations that implicitly encode meaning [18, 45], and are employed in building classifiers or text decoders, bypassing classic approaches to symbolic inference.\\nThis work is concerned with the problem of statutory reasoning [62, 66]: how to reason about an example situation, a case, based on complex rules provided in natural language. In addition to the reasoning aspect, we are motivated by the lack of contemporary systems to suggest legal opinions: while there exist tools to aid lawyers in retrieving relevant documents for a given case, we are unaware of any strong capabilities in automatic statutory reasoning. Our contributions, summarized in Figure 2, include a novel dataset based on US tax law, together with test cases (Section 2). Decades-old work in expert systems could solve problems of the sort we construct here, based on manually derived rules: we repli- cate that approach in a Prolog-based system that achieves 100% accuracy on our examples (Section 3). Our results demonstrate that straightforward application of contemporary Machine Read- ing models is not sufficient for our challenge examples (Section 5), whether or not they were adapted to the legal domain (Section 4). This is meant to provoke the question of whether we should be concerned with: (a) improving methods in semantic parsing in order to replace manual transduction into symbolic form; or (b) improving machine reading methods in order to avoid explicit symbolic solvers. We view this work as part of the conversation including recent work in multi-hop inference [61], where our task is more domain-specific but potentially more challenging.\\n1 INTRODUCTION Early artificial intelligence research focused on highly-performant, narrow-domain reasoning models, for instance in health [37, 40, 54] and law [30, 38]. Such expert systems relied on hand-crafted inference rules and domain knowledge, expressed and stored with the formalisms provided by databases [21]. The main bottleneck of this approach is that experts are slow in building such knowledge bases and exhibit imperfect recall, which motivated research into models for automatic information extraction (e.g. Lafferty et al. [36]). Systems for large-scale automatic knowledge base construction have improved (e.g. Etzioni et al. [20], Mitchell et al. [41]), as well as systems for sentence level semantic parsing [64]. Among others, this effort has led to question-answering systems for games [22] and, more recently, for science exams [14, 23, 27]. The challenges include extracting ungrounded knowledge from semi-structured\\n2 DATASET Here, we describe our main contribution, the StAtutory Reason- ing Assessment dataset (SARA): a set of rules extracted from the statutes of the US Internal Revenue Code (IRC), together with a set of natural language questions which may only be answered correctly by referring to the rules1.\\nThe IRC2 contains rules and definitions for the imposition and calculation of taxes. It is subdvided into sections, which in general,\\n1The dataset can be found under https://nlp.jhu.edu/law/ 2https://uscode.house.gov/browse/prelim@title26&edition=prelim\\nÂ§63. Taxable income Binary defined Â§2. Definitions and [special rules Â§1. Tax imposed Numerical (a) Married individuals filing joint returns land surviving spouses ns There is hereby imposed fion the jon the taxable income [ng of (1) every married individual...\\nFigure 1: Sample cases from our dataset. The questions can be answered by applying the rules contained in the statutes to the context. \\nus 26 case. law Prolog rules Tax Corpus Tax Vectors Legal BERT ae Â§ dodnaindn aa dodn\\nFigure 2: Resources. Corpora on the left hand side were used to build the datasets and models on the right hand side.\\ndefine one or more terms: section 3306 defines the terms employ- ment, employer and wages, for purposes of the federal unemploy- ment tax. Sections are typically structured around a general rule, followed by a number of exceptions. Each section and its subsec- tions may be cast as a predicate whose truth value can be checked against a state of the world. For instance, subsection 7703(a)(2):\\nan individual legally separated from his spouse under a decree of divorce or of separate maintenance shall not be considered as married\\ncan be checked given an individual.\\nSlots are another major feature of the law. Each subsection refers to a certain number of slots, which may be filled by existing entities (in the above, individual, spouse, and decree of divorce or of separate maintenance). Certain slots are implicitly filled: Â§7703(a)(1) and (b)(3) mention a â\\x80\\x9cspouse\", which must exist since the â\\x80\\x9cindividual\" is married. Similarly, slots which have been filled earlier in the section may be referred to later on. For instance, â\\x80\\x9chousehold\" is mentioned for the first time in Â§7703(b)(1), then again in Â§7703(b)(2) and in Â§7703(b)(3). Correctly resolving slots is a key point in successfully applying the law.\\nOverall, the IRC can be framed as a set of predicates formulated in human language. The language used to express the law has an open texture [29], which makes it particularly challenging for a computer- based system to determine whether a subsection applies, and to identify and fill the slots mentioned. This makes the IRC an excellent corpus to build systems that reason with rules specified in natural language, and have good language understanding capabilities.\\n2.1 Statutes and test cases As the basis of our set of rules, we selected sections of the IRC well-supported by Treasury Regulations, covering tax on individu- als (Â§1), marriage and other legal statuses (Â§2, 7703), dependents (Â§152), tax exemptions and deductions (Â§63, 68, 151) and employ- ment (Â§3301, 3306). We simplified the sections to (1) remove highly specific sections (e.g. those concerning the employment of sailors) in order to keep the statutes to a manageable size, and (2) ensure that the sections only refer to sections from the selected subset. For ease of comparison with the original statutes, we kept the original numbering and lettering, with no adjustment for removed sections. For example, there is a section 63(d) and a section 63(f), but no section 63(e). We assumed that any taxable year starts and ends at the same time as the corresponding calendar year.\\nFor each subsection extracted from the statutes, we manually created two paragraphs in natural language describing a case, one where the statute applies, and one where it does not. These snippets, formulated as a logical entailment task, are meant to test a systemâ\\x80\\x99s understanding of the statutes, as illustrated in Figure 1. The cases were vetted by a law professor for coherence and plausibility. For the purposes of machine learning, the cases were split into 176 train and 100 test samples, such that (1) each pair of positive and negative cases belongs to the same split, and (2) each section is split between train and test in the same proportions as the overall split. Since tax legislation makes it possible to predict how much tax a person owes, we created an additional set of 100 cases where the task is to predict how much tax someone owes. Those cases were created by randomly mixing and matching pairs of cases from the first set of cases, and resolving inconsistencies manually. Those cases are no longer a binary prediction task, but a task of predicting an integer. The prediction results from taking into account the entirety of the statutes, and involves basic arithmetic. The 100 cases were randomly split into 80 training and 20 test samples.\\nBecause the statutes were simplified, the answers to the cases are not those that would be obtained with the current version of the IRC. Some of the IRC counterparts of the statutes in our dataset have been repealed, amended, or adjusted to reflect inflation.\\n2.2 Key features of the corpus While the corpus is based on a simplification of the Internal Rev- enue Code, care was taken to retain prominent features of US law. We note that the present task is only one aspect of legal reason- ing, which in general involves many more modes of reasoning, in particular interpreting regulations and prior judicial decisions. The following features are quantified in Tables 1 to 4.\\nReasoning with time. The timing of events (marriage, retirement, income...) is highly relevant to determining whether certain sections apply, as tax is paid yearly. In total, 62 sections refer to time. Some sections require counting days, as in Â§7703(b)(1):\\nCross-references Within the section To another section Explicit 30 34 Implicit 25 44\\n# Table 1: Number of subsections containing cross-references\\nmin max 6 6 Table 2: Statistics about the tree structure of the statutes\\na household which constitutes for more than one-half of the taxable year the principal place of abode of a child or taking into account the absolute point in time as in Â§63(c)(7): In the case of a taxable year beginning after December 31, 2017, and before January 1, 2026-\\nExceptions and substitutions. Typically, each section of the IRC starts by defining a general case and then enumerates a number of exceptions to the rule. Additionally, some rules involve applying a rule after substituting terms. A total of 50 sections formulate an exception or a substitution. As an example, Â§63(f)(3):\\nIn the case of an individual who is not married and is not a surviving spouse, paragraphs (1) and (2) shall be applied by substituting â\\x80\\x9c$750\" for â\\x80\\x9c$600\".\\nNumerical reasoning. Computing tax owed requires knowledge of the basic arithmetic operations of adding, subtracting, multiplying, dividing, rounding and comparing numbers. 55 sections involve numerical reasoning. The operation to be used needs to be parsed out of natural text, as in Â§1(c)(2):\\n$3,315, plus 28% of the excess over $22,100 if the taxable income is over $22,100 but not over $53,500\\nCross-references. Each section of the IRC will typically reference other sections. Table 1 shows how this feature was preserved in our dataset. There are explicit references within the same section, as in Â§7703(b)(1):\\nan individual who is married (within the meaning of subsection (a)) and who files a separate return\\nexplicit references to another section, as in Â§3301:\\nThere is hereby imposed on every employer (as defined in section 3306(a)) for each calendar year an excise tax and implicit references, as in Â§151(a), where â\\x80\\x9ctaxable income\" is defined in Â§63:\\nthe exemptions provided by this section shall be allowed as deductions in computing taxable income.\\nCommon sense knowledge. Four concepts, other than time, are left undefined in our statutes: (1) kinship, (2) the fact that a marriage ends if either spouse dies, (3) if an event has not ended, then it is ongoing; if an event has no start, it has been true at any time before it ends; and some events are instantaneous (e.g. payments ), (4) a personâ\\x80\\x99s gross income is the sum of all income and payments received by that person.\\nHierarchical structure. Law statutes are divided into sections, themselves divided into subsections, with highly variable depth and structure. This can be represented by a tree, with a special ROOT node of depth 0 connecting all the sections. This tree contains 132 leaves and 193 nodes (node includes leaves). Statistics about depth are in Table 2.\\nVocabulary size Sentence length (in words) Case length (in sentences) Case length (in words) Section length train test statutes combined train test combined train test combined sentences words train test min max 138 34 88 138 9 7 9 179 81 179 16 1151 4 4 1 1 1 2 1 17 17 17 2 62 867 535 avg 12.3 11.6 16.5 12.7 4.2 3.8 4.1 48.5 41.6 46.3 8.3 488.9 statutes combined 768 1596 stddev median 11 10 12.5 11 4 4 4 43 38 41 9 549 9.1 4.5 14.9 9.5 1.7 1.3 1.6 22.2 14.7 20.3 4.7 310.4\\nTable 3: Language statistics. The word â\\x80\\x9ccombinedâ\\x80\\x9d means merging the corpora mentioned above it.\\ntrain test combined min 0 0 0 max 2,242,833 243,097 2,242,833 average 85,804.86 65,246.50 81,693.19 stddev 258,179.30 78,123.13 233,695.33 median 15,506.50 26,874.00 17,400.50 Table 4: Answers to numerical questions (in $).\\n3 PROLOG SOLVER It has been shown that subsets of statutes can be expressed in first- order logic, as described in Section 6. As a reaffirmation of this, and as a topline for our task, we have manually translated the statutes into Prolog rules and the cases into Prolog facts, such that each case can be answered correctly by a single Prolog query3. The Prolog rules were developed based on the statutes, meaning that the Prolog code clearly reflects the semantics of the textual form, as in Gunning et al. [27]. This is primarily meant as a proof that a carefully crafted reasoning engine, with perfect natural language understanding, can solve this dataset. There certainly are other ways of representing this given set of statutes and cases. The point of this dataset is not to design a better Prolog system, but to help the development of language understanding models capable of reasoning.\\n3.1 Statutes Each subsection of the statutes was translated with a single rule, true if the section applies, false otherwise. In addition, subsections define slots that may be filled and reused in other subsections, as described in Section 2. To solve this coreference problem, any term appearing in a subsection and relevant across subsections is turned into an argument of the Prolog rule. The corresponding variable may then be bound during the execution of a rule, and reused in a rule executed later. Unfilled slots correspond to unbound variables. To check whether a given subsection applies, the Prolog sys- tem needs to rely on certain predicates, which directly reflect the facts contained in the natural language descriptions of the cases. For instance, how do we translate Alice and Bob got married on January 24th, 1993 into code usable by Prolog? We rely on a set of 61 predicates, following neo-davidsonian semantics [9, 17, 42]. The level of detail of these predicates is based on the granularity of the statutes themselves. Anything the statutes do not define, and which is typically expressed with a single word, is potentially\\n3The Prolog program can be found under https://nlp.jhu.edu/law/\\nsuch a predicate: marriage, residing somewhere, someone paying someone else, etc. The example above is translated in Figure 3.\\n3.2 Cases The natural lan- guage description of each case was manually translated into the facts men- tioned above. The question or log- ical entailment prompt was translated into a Prolog query. For instance, Section 7703(b)(3) applies to Alice maintaining her home for the year 2018. translates to s7703_b_3(alice,home,2018). and How much tax does Alice have to pay in 2017? translates to tax(alice,2017,Amount).\\nIn the broader context of computational statutory reasoning, the Prolog solver has three limitations. First, producing it requires domain experts, while automatic generation is an open question. Second, translating natural language into facts requires semantic parsing capabilities. Third, small mistakes can lead to catastrophic failure. An orthogonal approach is to replace logical operators and explicit structure with high-dimensional, dense representations and real-valued functions, both learned using distributional statistics. Such a machine learning-based approach can be adapted to new legislation and new domains automatically.\\n4 LEGAL NLP As is commonly done in MR, we pretrained our models using two unsupervised learning paradigms on a large corpus of legal text.\\n4.1 Text corpus We curated a corpus consisting solely of freely-available tax law documents with 147M tokens. The first half is drawn from cas [1], a project of Harvardâ\\x80\\x99s Law Library that scanned and OCRâ\\x80\\x99ed many of the libraryâ\\x80\\x99s case-law reporters, making the text available upon request to researchers. The main challenge in using this resource is that it contains 1.7M U.S. federal cases, only a small percentage of which are on tax law (as opposed to criminal law, breach of contract, bankruptcy, etc.). Classifying cases by area is a non-trivial problem [55], and tax-law cases are litigated in many different courts. We used the heuristic of classifying a case as being tax-law if it met one of the following criteria: the Commissioner of Internal Revenue was a party; the case was decided by the U.S. Tax Court; or, the case was decided by any other federal court, other than a trade tribunal, with the United States as a party, and with the word tax appearing in the first 400 words of the caseâ\\x80\\x99s written opinion.\\nThe second half of this corpus consists of IRS private letter rul- ings and unpublished U.S. Tax Court cases. IRS private letter rulings are similar to cases, in that they apply tax law to one taxpayerâ\\x80\\x99s facts; they differ from cases in that they are written by IRS attorneys (not judges), have less precedential authority than cases, and redact names to protect taxpayer privacy. Unpublished U.S. Tax Court cases are viewed by the judges writing them as less important than those worthy of publication. These were downloaded as PDFs from the IRS and Tax Court websites, OCRâ\\x80\\x99ed with tesseract if needed, and otherwise cleaned.\\n4.2 Tax vectors Before training a word2vec model [39] on this corpus, we did two tax-specific preprocessing steps to ensure that semantic units re- mained together. First, we put underscores between multi-token collocations that are tax terms of art, defined in either the tax code, Treasury regulations, or a leading tax-law dictionary. Thus, â\\x80\\x9csurviv- ing spouse\" became the single token â\\x80\\x9csurviving_spouse\". Second, we turned all tax code sections and Treasury regulations into a single token, stripped of references to subsections, subparagraphs, and subclauses. Thus, â\\x80\\x9cTreas. Reg. Â§1.162-21(b)(1)(iv)\" became the single token â\\x80\\x9csec_1_162_21\". The vectors were trained at 500 dimen- sions using skip-gram with negative sampling. A window size of 15 was found to maximize performance on twelve human-constructed analogy tasks.\\n4.3 Legal BERT We performed further training of BERT [18], on a portion of the full case.law corpus, including both state and federal cases. We did not limit the training to tax cases. Rather, the only cases excluded were those under 400 characters (which tend to be summary orders with little semantic content) and those before 1970 (when judicial writing styles had become recognizably modern). We randomly selected a subset of the remaining cases, and broke all selected cases into chunks of exactly 510 tokens, which is the most BERTâ\\x80\\x99s architecture can handle. Any remaining tokens in a selected case were discarded. Using solely the masked language model task (i.e. not next sentence prediction), starting from Bert-Base-Cased, we trained on 900M tokens.\\nThe resulting Legal BERT has the exact same architecture as Bert-Base-Cased but parameters better attuned to legal tasks. We applied both models to the natural language questions and answers in the corpus we introduce in this paper. While Bert-Base-Cased had a perplexity of 14.4, Legal BERT had a perplexity of just 2.7, suggesting that the further training on 900M tokens made the model much better adapted to legal queries.\\nWe also probed how this further training impacted ability to handle fine-tuning on downstream tasks. The downstream task we chose was identifying legal terms in case texts. For this task, we defined legal terms as any tokens or multi-token collocations that are defined in Blackâ\\x80\\x99s Law Dictionary [25], the premier legal dictio- nary. We split the legal terms into training/dev/test splits. We put a 4-layer fully-connected MLP on top of both Bert-Base-Cased and Legal BERT, where the training objective was B-I-O tagging of tokens in 510-token sequences. We trained both on a set of 200M tokens randomly selected from case.law cases not previ- ously seen by the model and not containing any of the legal terms in dev or test, with the training legal terms tagged using string comparisons. We then tested both fine-tuned modelsâ\\x80\\x99 ability to identify legal terms from the test split in case law. The model based on Bert-Base-Cased achieved F1 = 0.35, whereas Legal BERT achieved F1 = 0.44. As a baseline, two trained lawyers given the same task on three 510-token sequences each achieved F1 = 0.26. These results indicate that Legal BERT is much better adapted to the legal domain than Bert-Base-Cased. Blackâ\\x80\\x99s Law Dictionary has well- developed standards for what terms are or are not included. BERT models learn those standards via the train set, whereas lawyers are not necessarily familiar with them. In addition, pre-processing\\ndropped some legal terms that were subsets of too many others, which the lawyers tended to identify. This explains how BERT- based models could outperform trained humans.\\n5 EXPERIMENTS 5.1 BERT-based models In the following, we frame our task as textual entailment and numer- ical regression. A given entailment prompt q mentions the relevant subsection (as in Figure 1)4. We extract s, the text of the relevant subsection, from the statutes. In q, we replace Section XYZ applies with This applies. We feed the string â\\x80\\x9c[CLS] + s + [SEP] + q + c + [SEP]\", where â\\x80\\x9c+\" is string concatenation, to BERT [18]. Let r be the vector representation of the token [CLS] in the final layer. The answer (entailment or contradiction) is predicted as Ð´(Î¸1 Â· r ) where Î¸1 is a learnable parameter and Ð´ is the sigmoid function. For numerical questions, all statutes have to be taken into account, which would exceed BERTâ\\x80\\x99s length limit. We encode â\\x80\\x9c[CLS] all [SEP] + q + c + [SEP]\" into r and predict the answer as Âµ + Ï\\x83Î¸2 Â· r where Î¸2 is a learned parameter, and Âµ and Ï\\x83 are the mean and standard deviation of the numerical answers on the training set.\\nFor entailment, we use a cross-entropy loss, and evaluate the models using accuracy. We frame the numerical questions as a taxpayer having to compute tax owed. By analogy with the concept of â\\x80\\x9csubstantial understatement of income taxâ\\x80\\x9d from Â§6662(d), we define â\\x88\\x86(y, Ë\\x86y) = max(0.1y,5000) where y is the true amount of tax owed, and Ë\\x86y is the taxpayerâ\\x80\\x99s prediction. The case â\\x88\\x86(y, Ë\\x86y) â\\x89¥ 1 corresponds to a substantial over- or understatement of tax. We compute the fraction of predictions Ë\\x86y such that â\\x88\\x86(y, Ë\\x86y) < 1 and report that as numerical accuracy.5 The loss function used is:\\nL= Y yilog gi + (1 - yi) log(1 - gi) + Y* max(A(yi, Gi) - 1,0) ieh iely\\nwhere I1 (resp. I2) is the set of entailment (resp. numerical) ques- tions, yi is the ground truth output, and Ë\\x86yi is the modelâ\\x80\\x99s output. We use Adam [34] with a linear warmup schedule for the learning rate. We freeze BERTâ\\x80\\x99s parameters, and experiment with unfreezing BERTâ\\x80\\x99s top layer. We select the final model based on early stopping with a random 10% of the training examples reserved as a dev set. The best performing model for entailment and for numerical questions are selected separately, during a hyperparameter search around the recommended setting (batch size=32, learning rate=1e- 5). To check for bias in our dataset, we drop either the statute, or the context and the statute, in which case we predict the answer from BERTâ\\x80\\x99s representation for â\\x80\\x9c[CLS] + c + [SEP] + q + [SEP]\" or â\\x80\\x9c[CLS] + q + [SEP]\", whichever is relevant.\\n5.2 Feedforward models We follow Arora et al. [2] to embed strings into vectors, with smoothing parameter equal to 10â\\x88\\x923. We use either tax vectors de- scribed in Section 4 or word2vec vectors [39]. We estimate unigram counts from the corpus used to build the tax vectors, or the train- ing set, whichever is relevant. For a given context c and question\\n4The code for these experiments can be found under https://github.com/SgfdDttt/sara 5For a company, a goal would be to have 100% accuracy (resulting in no tax penalties) while paying the lowest amount of taxes possible (giving them something of an interest- free loan, even if the IRS eventually collects the understated tax).\\nInputs - question context statutes statutes question context statutes statutes question context statutes question context statutes question context statutes question context statutes Table 5: Test set scores. We report the 90% confidence inter- val. All confidence intervals for entailment round to 8.3%.\\nor prompt q, we retrieve relevant subsection s as above. Using Arora et al. [2], s is mapped to vector vs , and (c, q) to vc+q . Let r = [vs , vq+c , |vs â\\x88\\x92 vc+q |, vs â\\x8a\\x99 vc+q ] where [a, b] is the concate- nation of a and b, |.| is the element-wise absolute value, and â\\x8a\\x99 is the element-wise product. The answer is predicted as Ð´(Î¸1 Â· f (r )) or Âµ + Ï\\x83Î¸2 Â· f (r ), as above, where f is a feed-forward neural net- work. We use batch normalization between each layer of the neural network [31]. As above, we perform ablation experiments, where we drop the statute, or the context and the statute, in which case r is replaced by vc+q or vq . We also experiment with f being the identity function (no neural network). Training is otherwise done as above, but without the warmup schedule.\\n5.3 Results We report the accuracy on the test set (in %) in Table 5. In our ab- lation experiments, â\\x80\\x9cquestion\" models have access to the question only, â\\x80\\x9ccontext\" to the context and question, and â\\x80\\x9cstatute\" to the statutes, context and question. For entailment, we use a majority baseline. For the numerical questions, we find the constant that minimizes the hinge loss on the training set up to 2 digits: $11,023. As a check, we swapped in the concatenation of the RTE datasets of Bentivogli et al. [5], Dagan et al. [16], Giampiccolo et al. [26], Haim et al. [28], and achieved 73.6% accuracy on the dev set with BERT, close to numbers reported in Wang et al. [59]. BERT was trained on Wikipedia, which contains snippets of law text: see article United States Code and links therefrom, especially Internal Revenue Code. Overall, models perform comparably to the baseline, independent of the underlying method. Performance remains mostly unchanged when dropping the statutes or statutes and context, meaning that models are not utilizing the statutes. Adapting BERT or word vec- tors to the legal domain has no noticeable effect. Our results suggest that performance will not be improved through straightforward application of a large-scale language model, unlike it is on other datasets: Raffel et al. [45] achieved 94.8% accuracy on COPA [49]\\nusing a large-scale multitask Transformer model, and BERT pro- vided a huge jump in performance on both SQuAD 2.0 [46] (+8.2 F1) and SWAG [63] (+27.1 percentage points accuracy) datasets as compared to predecessor models, pre-trained on smaller datasets. Here, we focus on the creation of resources adapted to the legal domain, and on testing off-the-shelf and historical solutions. Future work will consider specialized reasoning models.\\n6 RELATED WORK There have been several efforts to translate law statutes into expert systems. Oracle Policy Automation has been used to formalize rules in a variety of contexts. TAXMAN [38] focuses on corporate reorga- nization law, and is able to classify a case into three different legal types of reorganization, following a theorem-proving approach. Sergot et al. [52] translate the major part of the British National- ity Act 1981 into around 150 rules in micro-Prolog, proving the suitability of Prolog logic to express and apply legislation. Bench- Capon et al. [4] further discuss knowledge representation issues. Closest to our work is Sherman [53], who manually translated part of Canadaâ\\x80\\x99s Income Tax Act into a Prolog program. To our knowl- edge, the projects cited did not include a dataset or task that the programs were applied to. Other works have similarly described the formalization of law statutes into rule-based systems [24, 30, 32, 51]. Yoshioka et al. [62] introduce a dataset of Japanese statute law and its English translation, together with questions collected from the Japanese bar exam. To tackle these two tasks, Kim et al. [33] investigate heuristic-based and machine learning-based methods. A similar dataset based on the Chinese bar exam was released by Zhong et al. [66]. Many papers explore case-based reasoning for law, with expert systems [43, 56], human annotations [8] or automatic annotations [3] as well as transformer-based methods [44]. Some datasets are concerned with very specific tasks, as in tagging in contracts [10], classifying clauses [11], and classification of documents [12] or single paragraphs [6]. Ravichander et al. [47] have released a dataset of questions about privacy policies, elicited from turkers and answered by legal experts. Saeidi et al. [50] frame the task of statutory reasoning as a dialog between a user and a dialog agent. A single rule, with or without context, and a series of followup questions are needed to answer the original question. Contrary to our dataset, rules are isolated from the rest of the body of rules, and followup questions are part of the task.\\nClark et al. [14] describe a decades-long effort to answer science exam questions stated in natural language, based on descriptive knowledge stated in natural language. Their system relies on a variety of NLP and specialized reasoning techniques, with their most significant gains recently achieved via contextual language modeling. This line of work is the most related in spirit to where we believe research in statutory reasoning should focus. An interesting contrast is that while scientific reasoning is based on understanding the physical world, which in theory can be informed by all manner of evidence beyond texts, legal reasoning is governed by human- made rules. The latter are true by virtue of being written down and agreed to, and are not discovered through evidence and a scientific process. Thus, statutory reasoning is an exceptionally pure instance of a reasoner needing to understand prescriptive language.\\nWeston et al. [60] introduced a set of prerequisite toy tasks for AI systems, which require some amount of reasoning and common\\nsense knowledge. Contrary to the present work, the types of ques- tion in the train and test sets are highly related, and the vocabulary overlap is quite high. Numeric reasoning appears in a variety of MR challenges, such as in DROP [19].\\nUnderstanding procedural language â\\x80\\x93 knowledge needed to per- form a task â\\x80\\x93 is related to the problem of understanding statutes, and so we provide a brief description of some example investiga- tions in that area. Zhang et al. [65] published a dataset of how-to instructions, with human annotations defining key attributes (actee, purpose...) and models to automatically extract the attributes. Simi- larly, Chowdhury et al. [13] describe a dataset of human-elicited procedural knowledge, and WambsganÃ\\x9f and Fromm [58] automati- cally detect repair instructions from posts on an automotive forum. Branavan et al. [7] employed text from an instruction manual to improve the performance of a game-playing agent.\\n7 CONCLUSION We introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task. Our hand- built solver contrasts with our baselines based on current NLP approaches, even when we adapt them to the legal domain.\\nThe intersection between NLP and the legal domain is a growing area of research [3, 11, 33, 35, 48], but with few large-scale system- atic resources. Thus, in addition to the exciting challenge posed by statutory reasoning, we also intend this paper to be a contribution to legal-domain natural language processing.\\nGiven the poor out-of-the box performance of otherwise very powerful models, this dataset, which is quite small compared to typ- ical MR resources, raises the question of what the most promising direction of research would be. An important feature of statutory reasoning is the relative difficulty and expense in generating care- fully constructed training data: legal texts are written for and by lawyers, who are cost-prohibitive to employ in bulk. This is un- like most instances of MR where everyday texts can be annotated through crowdsourcing services. There are at least three strategies open to the community: automatic extraction of knowledge graphs from text with the same accuracy as we did for our Prolog solver [57]; improvements in MR to be significantly more data efficient in training; or new mechanisms for the efficient creation of training data based on pre-existing legal cases.\\nGoing forward, we hope our resource provides both (1) a bench- mark for a challenging aspect of natural legal language processing as well as for machine reasoning, and (2) legal-domain NLP models useful for the research community.\\nREFERENCES [1] 2019. Caselaw Access Project. http://case.law [2] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat\\nbaseline for sentence embeddings. (2016).\\n[3] Kevin D Ashley and Stefanie BrÃ¼ninghaus. 2009. Automatically classifying case texts and predicting outcomes. Artificial Intelligence and Law 17, 2 (2009), 125â\\x80\\x93165.\\n[4] Trevor JM Bench-Capon, Gwen O Robinson, Tom W Routen, and Marek J Sergot. 1987. Logic programming for large scale applications in law: A formalisation of supplementary benefit legislation. In Proceedings of the 1st international conference on Artificial intelligence and law. 190â\\x80\\x93198.\\n[5] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The Fifth PASCAL Recognizing Textual Entailment Challenge.. In TAC.\\n[6] Carlo Biagioli, Enrico Francesconi, Andrea Passerini, Simonetta Montemagni, and Claudia Soria. 2005. Automatic semantics extraction in law documents. In Proceedings of the 10th international conference on Artificial intelligence and law. 133â\\x80\\x93140.\\n[7] SRK Branavan, David Silver, and Regina Barzilay. 2012. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence Research 43 (2012), 661â\\x80\\x93704.\\n[8] Stefanie Bruninghaus and Kevin D Ashley. 2003. Predicting outcomes of case based legal arguments. In Proceedings of the 9th international conference on Artifi- cial intelligence and law. 233â\\x80\\x93242.\\n[9] Hector Neri CastaÃ±eda. 1967. Comment on D. Davidsonâ\\x80\\x99s â\\x80\\x9cThe logical forms of action sentencesâ\\x80\\x9d. The Logic of Decision and Action (1967).\\n[10] Ilias Chalkidis and Ion Androutsopoulos. 2017. A Deep Learning Approach to Contract Element Extraction.. In JURIX. 155â\\x80\\x93164.\\n[11] Ilias Chalkidis, Ion Androutsopoulos, and Achilleas Michos. 2018. Obligation and prohibition extraction using hierarchical rnns. arXiv preprint arXiv:1805.03871 (2018).\\n[12] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, and Ion Androut- sopoulos. 2019. Large-Scale Multi-Label Text Classification on EU Legislation. arXiv preprint arXiv:1906.02192 (2019).\\n[13] Debajyoti Paul Chowdhury, Arghya Biswas, Tomasz Sosnowski, and Kristina Yordanova. 2020. Towards Evaluating Plan Generation Approaches with Instruc- tional Texts. arXiv preprint arXiv:2001.04186 (2020).\\n[14] Peter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, et al. 2019. Fromâ\\x80\\x99Fâ\\x80\\x99toâ\\x80\\x99Aâ\\x80\\x99on the NY Regents Science Exams: An Overview of the Aristo Project. arXiv preprint arXiv:1909.01958 (2019).\\n[15] Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, et al. 1996. Using the framework. Technical Report. Technical Report LRE 62-051 D-16, The FraCaS Consortium.\\n[16] Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recog- nising textual entailment challenge. In Machine Learning Challenges Workshop. Springer, 177â\\x80\\x93190.\\n[17] Donald Davidson. 1967. The logical forms of action sentences. The Logic of Decision and Action (1967).\\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161 (2019). [20] Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the web. Commun. ACM 51, 12 (2008), 68â\\x80\\x9374. [21] Edward A Feigenbaum. 1992. Expert systems: principles and practice. (1992). [22] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building Watson: An overview of the DeepQA project. AI magazine 31, 3 (2010), 59â\\x80\\x9379.\\n[23] Noah S Friedland, Paul G Allen, Gavin Matthews, Michael Witbrock, David Baxter, Jon Curtis, Blake Shepard, Pierluigi Miraglia, Jurgen Angele, Steffen Staab, et al. 2004. Project halo: Towards a digital aristotle. AI magazine 25, 4 (2004), 29â\\x80\\x9329. [24] Wachara Fungwacharakorn and Ken Satoh. 2018. Legal Debugging in Proposi- tional Legal Representation. In JSAI International Symposium on Artificial Intelli- gence. Springer, 146â\\x80\\x93159.\\n[25] Bryan A Gardner. 2019. Blackâ\\x80\\x99s Law Dictionary (11 ed.). [26] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL- PASCAL workshop on textual entailment and paraphrasing. Association for Com- putational Linguistics, 1â\\x80\\x939.\\n[27] David Gunning, Vinay K Chaudhri, Peter E Clark, Ken Barker, Shaw-Yi Chaw, Mark Greaves, Benjamin Grosof, Alice Leung, David D McDonald, Sunil Mishra, et al. 2010. Project Halo UpdateÃ¢Ä\\x82Å¤Progress Toward Digital Aristotle. AI Maga- zine 31, 3 (2010), 33â\\x80\\x9358.\\n[28] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entail- ment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.\\n[29] Herbert Lionel Adolphus Hart and Herbert Lionel Adolphus Hart. 2012. The concept of law. Oxford university press.\\n[30] Robert Hellawell. 1980. A computer program for legal planning and analysis: Taxation of stock redemptions. Columbia Law Review 80, 7 (1980), 1363â\\x80\\x931398.\\n[31] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).\\n[32] Imran Khan, Muhammad Sher, Javed I Khan, Syed M Saqlain, Anwar Ghani, Husnain A Naqvi, and Muhammad Usman Ashraf. 2016. Conversion of legal text\\nto a logical rules set from medical law using the medical relational model and the world rule model for a medical decision support system. In Informatics, Vol. 3. Multidisciplinary Digital Publishing Institute, 2.\\n[33] Mi-Young Kim, Juliano Rabelo, and Randy Goebel. 2019. Statute Law Informa- tion Retrieval and Entailment. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law. 283â\\x80\\x93289.\\n[34] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\\n[35] Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A Corpus for Automatic Summarization of US Legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, Hong Kong, China, 48â\\x80\\x9356. https://doi.org/10.18653/v1/D19-5406\\n[36] John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. (2001).\\n[37] Robert S Ledley and Lee B Lusted. 1959. Reasoning foundations of medical diagnosis. Science 130, 3366 (1959), 9â\\x80\\x9321.\\n[38] L Thorne McCarty. 1976. Reflections on TAXMAN: An experiment in artificial intelligence and legal reasoning. Harv. L. Rev. 90 (1976), 837.\\n[39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111â\\x80\\x933119.\\n[40] Randolph A Miller, Harry E Pople Jr, and Jack D Myers. 1982. Internist-I, an experimental computer-based diagnostic consultant for general internal medicine. New England Journal of Medicine 307, 8 (1982), 468â\\x80\\x93476.\\n[41] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Betteridge, Andrew Carlson, Bhanava Dalvi, Matt Gardner, Bryan Kisiel, et al. 2018. Never-ending learning. Commun. ACM 61, 5 (2018), 103â\\x80\\x93115. [42] Terence Parsons. 1990. Events in the Semantics of English. Vol. 334. MIT press\\nCambridge, MA.\\n[43] Walter G Popp and Bernhard Schlink. 1974. Judith, a computer program to advise lawyers in reasoning a case. Jurimetrics J. 15 (1974), 303.\\n[44] Juliano Rabelo, Mi-Young Kim, and Randy Goebel. 2019. Combining Similarity and Transformer Methods for Case Law Entailment. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law. 290â\\x80\\x93296.\\n[45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim- its of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).\\n[46] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Donâ\\x80\\x99t Know: Unanswerable Questions for SQuAD. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.\\n[47] Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. 2019. Question Answering for Privacy Policies: Combining Computational and Legal Perspectives. arXiv preprint arXiv:1911.00841 (2019).\\n[48] Edwina L Rissland, Kevin D Ashley, and Ronald Prescott Loui. 2003. AI and Law: A fruitful synergy. Artificial Intelligence 150, 1-2 (2003), 1â\\x80\\x9315.\\n[49] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series.\\n[50] Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim RocktÃ¤schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading. arXiv preprint arXiv:1809.01494 (2018).\\n[51] Ken Satoh, Kento Asai, Takamune Kogawa, Masahiro Kubota, Megumi Naka- mura, Yoshiaki Nishigai, Kei Shirakawa, and Chiaki Takano. 2010. PROLEG: an implementation of the presupposed ultimate fact theory of Japanese civil code by PROLOG technology. In JSAI International Symposium on Artificial Intelligence. Springer, 153â\\x80\\x93164.\\n[52] Marek J. Sergot, Fariba Sadri, Robert A. Kowalski, Frank Kriwaczek, Peter Ham- mond, and H Terese Cory. 1986. The British Nationality Act as a logic program. Commun. ACM 29, 5 (1986), 370â\\x80\\x93386.\\n[53] David M Sherman. 1987. A Prolog model of the income tax act of Canada. In Proceedings of the 1st international conference on Artificial intelligence and law. 127â\\x80\\x93136.\\n[54] Edward H Shortliffe and Bruce G Buchanan. 1975. A model of inexact reasoning in medicine. Mathematical biosciences 23, 3-4 (1975), 351â\\x80\\x93379.\\n[55] Jerrold Soh, How Khang Lim, and Ian Ernst Chai. 2019. Legal Area Classification: A Comparative Study of Text Classifiers on Singapore Supreme Court Judgments. Association for Computational Linguistics, Minneapolis, Minnesota.\\n[56] Anne vdL Gardner. 1983. The design of a legal analysis program. In AAAI-83. 114â\\x80\\x93118.\\n[57] Lai Dac Viet, Vu Trong Sinh, Nguyen Le Minh, and Ken Satoh. 2017. ConvAMR: Abstract meaning representation parsing for legal document. arXiv preprint arXiv:1711.06141 (2017).\\n[58] Thiemo WambsganÃ\\x9f and HansjÃ¶rg Fromm. 2019. Mining User-Generated Repair Instructions from Automotive Web Communities. In Proceedings of the 52nd Hawaii International Conference on System Sciences.\\n[59] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier bench- mark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. 3261â\\x80\\x933275.\\n[60] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Mer- riÃ«nboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 (2015). [61] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 2369â\\x80\\x932380. https://doi.org/10. 18653/v1/D18-1259\\n[62] Masaharu Yoshioka, Yoshinobu Kano, Naoki Kiyota, and Ken Satoh. 2018. Overview of japanese statute law retrieval and entailment task at coliee-2018. In Twelfth International Workshop on Juris-informatics (JURISIN 2018).\\n[63] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large- scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326 (2018).\\n[64] Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019. Broad- Coverage Semantic Parsing as Transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3786â\\x80\\x933798. https://doi.org/ 10.18653/v1/D19-1392\\n[65] Ziqi Zhang, Philip Webster, Victoria S Uren, Andrea Varga, and Fabio Ciravegna. 2012. Automatically Extracting Procedural Knowledge from Instructional Texts using Natural Language Processing.. In LREC, Vol. 2012. 520â\\x80\\x93527.\\n[66] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2019. JEC-QA: A Legal-Domain Question Answering Dataset. arXiv preprint arXiv:1911.12011 (2019).',\n",
       " 'references': {'id': '1502.03167'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_loading:Starting to load 20 papers...\n",
      "INFO:data_loading:\n",
      "        Loading completed:\n",
      "        - Total papers processed: 20\n",
      "        - Successfully loaded: 20\n",
      "        - Failed to load: 0\n",
      "        - Time taken: 0.20 seconds\n",
      "        - Average rate: 101.24 papers/second\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "load_dataset_todb(\n",
    "    dataset\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see here structured data, as well as unstructured data (the content, and to a limited degree, authors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the candidate to create a pipeline to load this data into a database of their choice, ensuring the schema is optimized for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the database, to not spend TOO much time on making the datbase, I handled Database Normilzation only up to the First Normal Form (1NF). For projects needing more complicated and in-depth setup, we can explore 2NF, 3NF, ... 5NF or other normilization technique. Also decided to use sqlite for the database for the simplicity of creation in the demo. **They are all found in models.py**\n",
    "\n",
    "To optimize for querying, we handled the database with 1NF normalization, indexed frequent query columns, added constraints, and limited size of records.\n",
    "\n",
    "Finally I decided to keep the full 'content' on the Paper in case there is a use case of needing the exact document returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example:\n",
    "Session = get_or_create_database()\n",
    "    \n",
    "with Session() as session:\n",
    "    paper_obj = get_papers(session=session,limit=1)[0]\n",
    "    paper_dict = paper_obj.to_dict()\n",
    "\n",
    "paper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data may contain noise or require transformation (e.g., text cleaning, parsing nested JSON, handling missing values).\n",
    "\n",
    "### The candidate should demonstrate how they preprocess the data for efficient storage and later retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main player in data preprocessing is ArXivPreprocessor. It cleans up messy Unicode characters, removes unwanted special characters with regex, and converts raw academic papers into clean, standardized chunks. For storage efficiency, I've built in metadata tracking that includes token counts, document relationships (through pre/post chunk IDs), and citation references**\n",
    "\n",
    "**There are two speeds, 0 and 1. 0 is a slower, more expensive approach but uses much more advanced algorithms to chunk the data (StatisticalChunking) otherwise we use a simple RollingWindowSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bababako/side_projects/llm_projects/disney/RAG_Example_DSNY/preprocessor.py:47: UserWarning: Splitters are being deprecated. They have moved to their own package. Please migrate to the `semantic-chunkers` package. More information can be found at:\n",
      "https://github.com/aurelio-labs/semantic-chunkers\n",
      "  self.splitter = RollingWindowSplitter(\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArXivPreprocessor(encoder=encoder,speed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-05 18:58:34 INFO semantic_router.utils.logger Single document exceeds the maximum token limit of 500. Splitting to sentences before semantically splitting.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:preprocessor:Successfully preprocessed paper 2401.04088 into 41 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Statistics:\n",
      "  - Total Documents: 238\n",
      "  - Total Splits: 41\n",
      "  - Splits by Threshold: 34\n",
      "  - Splits by Max Chunk Size: 6\n",
      "  - Last Split: 1\n",
      "  - Minimum Token Size of Split: 43\n",
      "  - Maximum Token Size of Split: 482\n",
      "  - Similarity Split Ratio: 0.83\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "with Session() as session:\n",
    "    chunks_data = arxiv.preprocess_paper(paper_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorization:\n",
    "\n",
    "##### 1. Using a pre-trained language model or embeddings model, ask the candidate to convert the unstructured text into embeddings.\n",
    "##### 2. Store these embeddings in a vector storage solution of their choice, ensuring the pipeline can handle batch processing for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I've built a Vectorizer class that takes any embedding model and converts our preprocessed chunks into vectors, storing them in Pinecone with all our metadata intact and linked. Everything's built to handle scale - it processes in configurable batches using a batch_generator, has error handling, and shows real-time progress bars. Plus, storing vectors with their full context makes retrieval super smart later!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(encoder=encoder,index=index,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 1/9 [00:01<00:15,  1.93s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 2/9 [00:03<00:10,  1.55s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 3/9 [00:04<00:07,  1.24s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 4/9 [00:05<00:06,  1.21s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 56%|█████▌    | 5/9 [00:06<00:04,  1.16s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 67%|██████▋   | 6/9 [00:07<00:03,  1.09s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 7/9 [00:08<00:02,  1.01s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 8/9 [00:08<00:00,  1.07it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 9/9 [00:09<00:00,  1.04s/it]\n",
      "INFO:vectorizer:Vectorization completed:\n",
      "- Processed chunks: 41\n",
      "- Failed chunks: 0\n",
      "- Processing time: 9.45 seconds\n",
      "- Rate: 4.34 chunks/second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_chunks': 41,\n",
       " 'processed_chunks': 41,\n",
       " 'failed_chunks': 0,\n",
       " 'processing_time': 9.452132,\n",
       " 'chunks_per_second': 4.337645729027059}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "vectorizer.vectorize_and_store(metadata_list=chunks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query and Retrieve:\n",
    "\n",
    "##### 1. Create a simple API or script that allows querying based on a given text prompt.The query should retrieve similar embeddings from the vector store and return the corresponding records from the database.\n",
    "\n",
    "##### 2. Include a use case for Retriever-Augmented Generation (RAG), where the retrieved data is used to generate a summary or response based on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There was a very simple way of avoiding the use of agents and custom vectorization by using langchain retrievers and vectorstores. Taking this approach would've let to a faster rag conversation. But I wanted to demonstrate my ability of making custom agents, and leaning towards my understanding of custom tools and control of the RAG environment. Albeit, this is still a extremely simple example of agent creation, and if needed for more complex demonstrations reach out to me through eneadodi.com** \n",
    "\n",
    "**Finally, because we have a database with very important information related to summary, citations, authors, etc we can use agents for things like:**\n",
    "* finding documents of authors given a document used in response\n",
    "* finding documents that cite the current document used in response\n",
    "* finding and querying by category for comparison\n",
    "\n",
    "**These can all be done by simply adding more tools on the tools list below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool \n",
    "def get_rag_context(query:str)->list[dict]:\n",
    "    \"\"\"\n",
    "    Helper Tool Function that should always be called for an input by the user to get context.\n",
    "    \"\"\"\n",
    "    return vectorizer.query(query)\n",
    "\n",
    "tools = [get_rag_context]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a helpful AI assistant with access to relevant context through the get_rag_context tool. \n",
    "        Always use this tool first to retrieve context before answering questions.\n",
    "        After getting context, use it to provide accurate and informed responses.\n",
    "        If the context doesn't contain relevant information, acknowledge that and provide a general response.\n",
    "        The context is largely in relation to science publications, so you know how to summarize those as an expert.\n",
    "        Get it right and you get $100 tip!\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_rag_context` with `{'query': 'algorithms used to find the results of the risk'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[{'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'ly, AC LL and RF-C LL for the combinations of AC and RF-C with LL. Character Error Rate AC LL 17.24 17.81 17.31 18.4 35.89 38.12 37.0 40.87 L 10, Î 0.3 L 30, Î 0.3 L 10, Î 0.5 L 30, Î 0.5 RF-C AC LL RF-C LL 17.82 18.16 35.84 37.6 16.65 17.1 34.6 36.36 16.97 17.47 35 36.6 Table 2: Our IWSLT 2014 machine translation results with a convolutional encoder compared to the previous work by Ranzato et al. Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work. The training data comprises about 153,000 German-English sentence pairs. In addition we considered a larger WMT14 English-French dataset Cho et al. (2014) with more than 12 million examples. For further information about the data we refer the reader to Appendix B. The return is deï ned as a smoothed and rescaled version of the BLEU score. Speciï cally, we start all n-gram counts from 1 instead of ', 'score': 0.353827566, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 17.0, 'content': 'Please see 1 for an explanation of abbreviations. The asterisk identiï es results from (Ranzato et al., 2015). The numbers reported with â were approximately read from Figure 6 of (Ranzato et al., 2015) Decoding method greedy search beam search LL MIXER 17.74 â 20.3 20.73 â 21.9 RF 20.92 21.35 RF-C 22.24 22.58 AC 21.66 22.45 out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofï cient Î LL 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic. 5.2 MACHINE TRANSLATION For our ï rst translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work.', 'postchunk_id': '1607.07086#18', 'prechunk_id': '1607.07086#16', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 246.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'm the pre-trained actor, while the actorâ s parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2. 4 RELATED WORK In other recent RL-inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results. Another recently proposed method is to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure itself (Wiseman Rush, 2016). This method follows the more general strategy called â learning as search optimizationâ (DaumÂ e III Marcu, 2005). This is an interesting alternative to our approach; however, it is designed speciï cally for the precise ', 'score': 0.350776404, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 12.0, 'content': 'However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the ground-truth for value prediction. Imitation learning has also been applied to structured prediction (Vlachos, 2012). Methods of this type include the SEARN (DaumÂ e Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, itâ s not always easy or even possible to construct an expert policy for a task-speciï c score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-speciï c score. The recently proposed â scheduled samplingâ (Bengio et al., 2015) can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy. There are also approaches that aim to approximate the gradient of the expected score. One such approach is â Direct Loss Minimizationâ (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-speciï c score into account. Another popular approach is to replace the domain over which the task score expectation is deï ned with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel Byrne, 2000; Shen et al., 2015; Och, 2003). This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and Shen et al. (2015) report that as many as 100 samples were required for the best results.', 'postchunk_id': '1607.07086#13', 'prechunk_id': '1607.07086#11', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 481.0}}, {'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'content': 'Please see Table 1 for an explanation of abbreviations. The asterisk identiï es results from (Wiseman Rush, 2016). Model greedy search beam search LL 22.53 23.87 BSO 23.83 25.48 LL 25.82 27.56 RF-C RF-C LL 27.42 27.75 27.7 28.3 AC 27.27 27.75 AC LL 27.49 28.53 Table 4: Our WMT 14 machine translation results compared to the previous work. Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset. In addition, we performed an ablation study. We found that using a target network was crucial; while the joint actor-critic training was still progressing with Î3Î 0.1, with Î3Î 1.0 it did not work at all. Similarly important was the value penalty described in Equation (10). We found that good values of the Î coefï cient were in the range [10â 3, 10â 6]. Other techniques, such as reward shaping an', 'score': 0.346584976, 'metadata': {'arxiv_id': '1607.07086', 'chunk_index': 21.0, 'content': 'Please see Table 1 for an explanation of abbreviations. The apostrophy and the asterisk identify results from (Bahdanau et al., 2015) and (Shen et al., 2015) respectively. Decoding method greedy search beam search LLâ n a 28.45 LL MRT n a 29.88 n a 31.3 Model LL 29.33 30.71 AC LL RF-C LL 30.85 31.13 29.83 30.37 points stronger than its equivalent from (Ranzato et al., 2015). The performance of REINFORCE with a simple baseline matches the score reported for MIXER in Ranzato et al. (2015). To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in Figure 2. We can clearly see that the training methods that use generated predictions have a strong regularization effect â that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of ï tting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overï tting, which is a healthy behaviour for such a small dataset.', 'postchunk_id': '1607.07086#22', 'prechunk_id': '1607.07086#20', 'references': [], 'title': 'An Actor-Critic Algorithm for Sequence Prediction', 'token_count': 286.0}}]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe context retrieved does not provide specific information about algorithms used to find the results of risk in a particular study. However, it does mention various algorithms related to sequence prediction and machine translation, such as:\n",
      "\n",
      "1. **Actor-Critic Algorithm**: This is a reinforcement learning algorithm used for sequence prediction, which combines the benefits of both actor and critic networks.\n",
      "\n",
      "2. **REINFORCE Algorithm**: A policy gradient method used in reinforcement learning, known for its high variance.\n",
      "\n",
      "3. **SARSA and OLPOMDP**: Standard value-based reinforcement learning algorithms applied to structured prediction.\n",
      "\n",
      "4. **Imitation Learning Algorithms**: Such as SEARN and DAGGER, which rely on an expert policy to provide action sequences for the policy to imitate.\n",
      "\n",
      "5. **Minimum (Bayes) Risk Training**: An approach that replaces the domain over which the task score expectation is defined with a small subset, typically an n-best list or a sample.\n",
      "\n",
      "If you have a specific study or context in mind regarding risk assessment, please provide more details so I can assist you better.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=True)\n",
    "output = agent.query(\"what algorithms were used to find the results of the risk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FULL EXAMPLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load DIRECTLY from the database.\n",
    "stats= process_existing_papers(preprocessor=arxiv,vectorizer=vectorizer,db_url='sqlite:///arxiv_papers.db',stop_at=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_papers': 10, 'processed_papers': 10, 'failed_papers': 0, 'processing_time': 186.601471, 'papers_per_second': 0.053590145599656074}\n"
     ]
    }
   ],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RAGAgent(vectorizer=vectorizer,tools=tools,prompt=prompt,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:  what were the algorithms used to detect the abnormality?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "\n",
      "🤖The algorithms used for detecting abnormalities, particularly in the context of machine-generated text and out-of-distribution (OOD) detection, include:\n",
      "\n",
      "1. **Post-hoc Detection Models**: These models perform a post-hoc analysis of machine-generated text using language model features or by fine-tuning existing large language models to behave as detectors. They work by identifying detectable signals left by language models in generated text.\n",
      "\n",
      "2. **Maximum Softmax Probability (MSP)**: This method is used for OOD detection. It classifies whether a given test point belongs to an in-distribution dataset or an out-of-distribution dataset by evaluating the maximum softmax probability.\n",
      "\n",
      "3. **Actor-Critic Algorithm**: This reinforcement learning algorithm is used for sequence prediction tasks, such as recovering strings of natural text from corrupted versions. It involves training models to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure.\n",
      "\n",
      "These algorithms are applied in various contexts, such as detecting machine-generated text, classifying images with algorithmically generated corruptions, and sequence prediction tasks like spelling correction and machine translation.\n",
      "\n",
      "----------------------------\n",
      "👋 Catch you later!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"🔍 Ask me anything (or 'exit()' to quit): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit()':\n",
    "        print(\"👋 Catch you later!\")\n",
    "        agent.clear_chat_history\n",
    "        break\n",
    "    print(\"INPUT: \", user_input)\n",
    "    try:\n",
    "        output = agent.query(user_input)\n",
    "        print(\"----------------------------\\n\")\n",
    "        print(\"🤖\"+output)\n",
    "        print(\"\\n----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Oops! Something went wrong: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Documentation:\n",
    "\n",
    "##### a. The candidate should document their code, the thought process behind their design choices, and any trade-offs they considered (e.g., schema design, vector storage approach, etc.).\n",
    "\n",
    "\n",
    "**Documentation, including explanations of design choices, and code descriptions are found on the jupyter notebook and the files.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "# 6. Bonus:\n",
    "\n",
    "#####    Implement monitoring or logging for the data pipeline to track the data flow and identify potential bottlenecks.\n",
    "#####    Optimize the pipeline for scalability, such as handling larger files or parallel processing.\n",
    "\n",
    "\n",
    "**logging is implemented in all key aspects of preprocessing, vectorization, and data flow. Pipeline is optimized for scalability by handling work through batches where we handle offsets in a way that allows parallelization. Of course more work needs to be done to really use the batch handling correctly, and turning it into async functions as well. But for this very simple demonstration, I decided to leave out that work. If needed, reach out to me on eneadodi.com to ask for more complicated examples** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
